{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79bdddd4",
   "metadata": {},
   "source": [
    "# CS 336 Lecture 3.1 Architecture for Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3ee50f",
   "metadata": {},
   "source": [
    "Starting with the original transformer...\n",
    "\n",
    "- (Masked) MultiHead Attention\n",
    "\n",
    "- Positional Embeddings\n",
    "\n",
    "- FFN (MLP), with RELU activation function.\n",
    "\n",
    "- LayerNorm\n",
    "\n",
    "However, the original transformer is designed for machine translation, which are viewed as a variant of RNNs. Nowadays, different models have different architecture design in detail:\n",
    "\n",
    "| Name | Year | LayerNorm | Parallel Layer | Pre-norm | Position embedding | Activations | Stability tricks |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Original transformer | 2017 | LayerNorm | Serial | ❌ | Sine | ReLU | |\n",
    "| GPT | 2018 | LayerNorm | Serial | ❌ | Absolute | GeLU | |\n",
    "| T5 (11B) | 2019 | RMSNorm | Serial | ✅ | Relative | ReLU | |\n",
    "| GPT2 | 2019 | LayerNorm | Serial | ❌ | Absolute | GeLU | |\n",
    "| GPT2 | 2020 | RMSNorm | Serial | ✅ | Relative | GeGLU | |\n",
    "| T5 (XXL 11B) v1.1 | 2020 | RMSNorm | Serial | ✅ | Relative | GeGLU | |\n",
    "| mT5 | 2020 | RMSNorm | Serial | ✅ | Relative | GeGLU | |\n",
    "| GPT3 (175B) | 2020 | LayerNorm | Serial | ✅ | Absolute | GeLU | |\n",
    "| GPTJ | 2021 | LayerNorm | Parallel | ✅ | RoPE | GeLU | |\n",
    "| LaMDA | 2021 | | | | Relative | GeGLU | |\n",
    "| Anthropic LM (not claude) | 2021 | | | | | | |\n",
    "| Gopher (280B) | 2021 | RMSNorm | Serial | ✅ | Relative | ReLU | |\n",
    "| GPT-NeoX | 2022 | LayerNorm | Parallel | ✅ | RoPE | GeLU | |\n",
    "| BLOOM (175B) | 2022 | LayerNorm | Parallel | ✅ | Alibi | GeLU | |\n",
    "| OPT (175B) | 2022 | LayerNorm | Serial | ❌ | Absolute | ReLU | |\n",
    "| PaLM (540B) | 2022 | RMSNorm | Parallel | ✅ | RoPE | SwiGLU | Z-loss |\n",
    "| Chinchilla | 2022 | RMSNorm | Serial | ✅ | Relative | ReLU | |\n",
    "| Mistral (7B) | 2023 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| LLaMA2 (70B) | 2023 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| LLaMA (65B) | 2023 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| GPT4 | 2023 | | | ❌ | | | |\n",
    "| Olmo 2 | 2024 | RMSNorm | Serial | ❌ | RoPE | SwiGLU | Z-loss, QK-norm |\n",
    "| Gemma 2 (27B) | 2024 | RMSNorm | Serial | ✅ | RoPE | GeGLU | Logit soft capping, Pre+post norm |\n",
    "| Nemotron-4 (340B) | 2024 | LayerNorm | Serial | ✅ | RoPE | SqReLu | |\n",
    "| Qwen 2 (72B) - same for 2.5 | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Falcon 2 11B | 2024 | LayerNorm | Parallel | ✅ | RoPE | GeLU | Z-loss |\n",
    "| Ph3 (small) - same for ph4 | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Llama 3 (70B) | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Reka Flash | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Command R+ | 2024 | LayerNorm | Parallel | ✅ | RoPE | SwiGLU | |\n",
    "| OLMo | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Qwen (14B) | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| DeepSeek (67B) | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Yi (34B) | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Mixtral of Experts | 2024 | | | ❌ | | | |\n",
    "| Command A | 2025 | LayerNorm | Parallel | ✅ | Hybrid (RoPE+NoPE) | SwiGLU | |\n",
    "| Gemma 3 | 2025 | RMSNorm | Serial | ✅ | RoPE | GeGLU | Pre+post norm, QK-norm |\n",
    "| SmolLM2 (1.7B) | 2025 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad97fdf",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1d929",
   "metadata": {},
   "source": [
    "### PreNorm vs PostNorm\n",
    "\n",
    "![Norm](../../assets/imgs/norm.png)\n",
    "\n",
    "Several discussions for PreNorm and PostNorm:\n",
    "\n",
    "- [Discussion](https://kexue.fm/archives/9009)\n",
    "\n",
    "**TLDR**: With the same parameter settings, pre-norm can be trained more easily than post-norm, however, the final performance does not outweigh post-norm.(or exactly the same for the pre-training process.)\n",
    "\n",
    "However, for the post-training fine-tuning, the post-normalization architecture has better transfer performance.\n",
    "\n",
    "> However, the gradients of Pre-LN at bottom layers tend to be larger than at top layers, leading to a degradation in performance compared with Post-LN.\n",
    "\n",
    "But for pre-norm, which can keep the good parts of residual connections, with nicer gradient propagation and fewer spike."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2eb4e",
   "metadata": {},
   "source": [
    "Maybe we can do Double Norm ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a95306",
   "metadata": {},
   "source": [
    "### Layer Norm & RMS Norm\n",
    "\n",
    "#### Layer Normalization\n",
    "\n",
    "在层归一化中，我们沿着 **特征维度 ($D$)** 计算统计量，对每个 **样本 ($N$)** 和 **序列位置 ($L$)** 进行归一化。\n",
    "这意味着 $\\mu$ 和 $\\sigma^2$ 是针对每个样本 $n$ 和每个序列位置 $l$ 计算的。\n",
    "\n",
    "* **均值 $\\mu_{n,l}$：**\n",
    "    $$\\mu_{n,l} = \\frac{1}{D} \\sum_{d=1}^D x_{n,l,d}$$\n",
    "* **方差 $\\sigma_{n,l}^2$：**\n",
    "    $$\\sigma_{n,l}^2 = \\frac{1}{D} \\sum_{d=1}^D (x_{n,l,d} - \\mu_{n,l})^2$$\n",
    "* **缩放因子和平移因子：** $\\gamma$ 和 $\\beta$ 在所有样本和序列位置上共享，但通常也是与特征维度 $D$ 相同长度的向量，即 $\\gamma, \\beta \\in \\mathbb{R}^{D}$。\n",
    "    因此，对于每个 $x_{n,l,d}$：\n",
    "    $$y_{n,l,d} = \\gamma_d \\left( \\frac{x_{n,l,d} - \\mu_{n,l}}{\\sqrt{\\sigma_{n,l}^2 + \\epsilon}} \\right) + \\beta_d$$\n",
    "\n",
    "> 在 Transformer 等序列建模模型中，因为序列长度 $N$ 可变（并且是频繁变化），因此为了提升模型的鲁棒性，往往采用 **Layer Normalization** 的方式实现归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d131709",
   "metadata": {},
   "source": [
    "#### RMS Norm\n",
    "\n",
    "RMSNorm, which stands for **Root Mean Square Normalization**, is a layer normalization technique used in some transformer architectures. It's a simpler and more computationally efficient alternative to traditional Layer Normalization.\n",
    "\n",
    "The core principle of RMSNorm is to **re-scale the activations of a layer based on their root mean square (RMS)** value, rather than their mean and standard deviation.\n",
    "\n",
    "For a given input vector $x$ (the activations from a layer), the RMSNorm operation is defined as:\n",
    "\n",
    "> The normalization also happens in the dimensional feature.\n",
    "\n",
    "$$y_{n,l,d} = \\gamma_d \\left( \\frac{x_{n,l,d}}{\\sqrt{\\frac{1}{D} \\sum_{i=1}^{D} x_{n,l,i}^2 + \\epsilon}} \\right)$$\n",
    "\n",
    "\n",
    "**RMSNorm** calculates the root mean square, which is essentially the L2 norm divided by the square root of the dimension. This method is simpler as it **doesn't subtract the mean**, which is the most computationally expensive part of Layer Normalization. This makes RMSNorm more **efficient and faster**, especially in large-scale models (Runtime: Memory Movement). It normalizes the magnitude of the activations, which is often sufficient for stabilizing training.\n",
    "\n",
    "- also: it drops bias terms for memory and optimization stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213a2fd",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "For non-linear components in the neural network.\n",
    "\n",
    "### GLU (Gated Activations)\n",
    "\n",
    "Original Feed Forward Network: $FF(x) = \\max(0, xW_1)W_2$.\n",
    "\n",
    "With GLU:\n",
    "\n",
    "$$\\max(0, xW_1) \\to \\max(0, xW_1) \\otimes (xV)$$\n",
    "\n",
    "$\\otimes$ means element-wise operations, the former part of GLU is also the RELU function, and the latter let the input $x$ passing through the matrix $V$ and controls which message can be passed into this successfully.\n",
    "\n",
    "$$\\text{FF}_{\\text{REGLU}}(x) = (\\max(0, xW_1) \\otimes (xV) ) W_2$$\n",
    "\n",
    "### SwiGLU Mathematical Formula\n",
    "\n",
    "SwiGLU, which stands for **Swish-Gated Linear Unit**, is a variant of the Gated Linear Unit (GLU) family. It was introduced by Google Brain in the PaLM model and has since been adopted by other modern large language models, such as LLaMA, as the activation function within their feed-forward networks (FFN). It has been shown to outperform both ReLU and GeLU in terms of performance.\n",
    "\n",
    "SwiGLU functions as a sub-module within the feed-forward network. Its core idea is to **replace the traditional single-path activation with a two-path structure controlled by a \"gate.\"**.\n",
    "\n",
    "**SwiGLU-based FFN**:\n",
    "$$FFN(x) = \\text{SwiGLU}(xW_1, xW_2)W_3$$\n",
    "\n",
    "Here, $x$ is the input vector, and $W_1, W_2, W_3$ are learnable weight matrices.\n",
    "\n",
    "The core operation of SwiGLU is defined as:\n",
    "\n",
    "$$\\text{SwiGLU}(x, W_1, W_2) = (\\text{Swish}(xW_1)) \\otimes (xW_2)$$\n",
    "\n",
    "Where:\n",
    "* $x$ is the input vector.\n",
    "* $W_1$ and $W_2$ are two distinct weight matrices.\n",
    "* **$\\text{Swish}(\\cdot)$** is the activation function defined by:\n",
    "  $$\\text{Swish}(z) = z \\cdot \\sigma(z)$$\n",
    "  where $\\sigma(z)$ is the Sigmoid function:\n",
    "  $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "* **$\\otimes$** denotes **element-wise multiplication**.\n",
    "\n",
    "1.  **Path 1 (Activation Path)**: The input vector $x$ is multiplied by the weight matrix $W_1$ to produce a vector $z_1 = xW_1$. This vector is then passed through the Swish activation function: Swish($z_1$).\n",
    "2.  **Path 2 (Gating Path)**: The input vector $x$ is multiplied by a separate weight matrix $W_2$ to produce another vector $z_2 = xW_2$.\n",
    "3.  **Gating**: The result from Path 1, Swish($z_1$), is multiplied element-wise by the result from Path 2,$z_2$.\n",
    "\n",
    "This element-wise multiplication is the key \"gating\" mechanism. The second path, $z_2 = xW_2$, acts as a gate, dynamically adjusting the weight of each element in the activated vector from Path 1 based on the input $x$.\n",
    "\n",
    "* **Dynamic Information Flow**: The gating design allows the network to learn which features should be amplified or suppressed, providing a more precise way to control information flow. This adaptive gating ability helps the model better capture complex dependencies.\n",
    "* **Smoother Gradients**: Unlike ReLU's hard cutoff (with a gradient of 0 or 1), the Swish function has a smooth gradient. This helps in mitigating the vanishing gradient problem in deep networks, leading to more stable optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f19641",
   "metadata": {},
   "source": [
    "## Serial vs Parallel layers\n",
    "\n",
    "Original serialized attention mechanism cannot do parallel computations.\n",
    "\n",
    "- $y = x + \\text{MLP}(\\text{LayerNorm}(x + \\text{Attention}(\\text{LayerNorm}(x))))$\n",
    "\n",
    "- $y = x + \\text{MLP}(\\text{LayerNorm}(x)) + \\text{Attention}(\\text{LayerNorm}(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecfbeca",
   "metadata": {},
   "source": [
    "## Positional Embeddings\n",
    "\n",
    "### Dive Deeper into Sine Embeddings\n",
    "\n",
    "Sine Embeddings, 或者叫做 Sinusoidal Embeddings，是一种对输入向量进行位置编码的方式，这种**基于正弦函数**的位置编码也是最先在 Attention is all you need 论文中提出的方法。\n",
    "\n",
    "下文的一些摘录节选自 [Transformer 升级之路](https://spaces.ac.cn/archives/9675)，并且加上自己的注解。\n",
    "\n",
    "位置编码重要性不必多言，在自回归模型 Attention Score 计算的过程中，其模型是**全对称的**，满足恒等式 $f(x,y) = f(y,x)$，这会对最终的序列预测任务造成困扰，因此，需要引入**位置编码**在学习相关性的基础上学习不同词元的序列特征。而这里的序列特征指的是**绝对序列特征**和**相对序列特征**。\n",
    "\n",
    "为了打破对称性，位置编码改变了自注意力层的函数输入：\n",
    "\n",
    "$$f'(x) = (\\cdots, x_m, \\cdots, x_n, \\cdots) = f(\\cdots, x_m + p_m, \\cdots, x_n+p_n, \\cdots) $$\n",
    "\n",
    "> 这里采用**加**作为注入位置信息的方式，计算简单并且容易学习，但是也会在训练过程中带来问题，后面会详细讲到这个点。\n",
    "\n",
    "只考虑这两个位置，并进行泰勒展开：\n",
    "\n",
    "$$\\tilde{f} \\approx f + \\boldsymbol{p}_m^\\top \\frac{\\partial f}{\\partial \\boldsymbol{x}_m} + \\boldsymbol{p}_n^\\top \\frac{\\partial f}{\\partial \\boldsymbol{x}_n} + \\frac{1}{2} \\boldsymbol{p}_m^\\top \\frac{\\partial^2 f}{\\partial \\boldsymbol{x}_m^2} \\boldsymbol{p}_m + \\frac{1}{2} \\boldsymbol{p}_n^\\top \\frac{\\partial^2 f}{\\partial \\boldsymbol{x}_n^2} \\boldsymbol{p}_n + \\underbrace{\\boldsymbol{p}_m^\\top \\frac{\\partial^2 f}{\\partial \\boldsymbol{x}_m \\partial \\boldsymbol{x}_n} \\boldsymbol{p}_n}_{\\boldsymbol{p}_m^\\top \\mathcal{H} \\boldsymbol{p}_n}$$\n",
    "\n",
    "之前的项要么只有 $p_m$，要么只有 $p_n$，要么两个都没有（注意，只有$p$存储了位置信息，$x_n$, $x_m$本身只存储了语义信息），因此表达的是绝对信息。（因为只依赖于单一位置），而最后一项同时出现了 $p_m$ 和 $p_n$，因此这是**模型存储相对位置信息的关键**。\n",
    "\n",
    "$\\frac{\\partial^2 f}{\\partial \\boldsymbol{x}_m \\partial \\boldsymbol{x}_n}$ 这一项记为矩阵 $\\mathcal{H}$，因此我们的目标项是 $p_m^\\top \\mathcal{H} p_n = g(m-n)$，而我们需要找到符合条件的编码 $p_m$, $p_n$。\n",
    "\n",
    "> 当 $\\mathcal{H}$ 为对角矩阵的时候，下面的解可以严格的被表示为 $g(m-n)$，但是对于一般的矩阵，这样的性质是不成立的，我们只能希望 $\\mathcal{H}$ 的对角线占主元。\n",
    "\n",
    "具体的计算过程详见：[Original Blog](https://spaces.ac.cn/archives/8231)，简直是极为精彩的推导过程！最终，我们从 $d = 2$ 的简单复数情形出发，一步步推导到高维偶数编码的一个可行解：\n",
    "\n",
    "$$p_m = \\begin{pmatrix}\n",
    " \\cos m\\theta_0\n",
    " \\\\ \\sin m \\theta_0\n",
    " \\\\ \\cos m \\theta_1\n",
    " \\\\ \\sin n \\theta_1\n",
    " \\\\ \\cdots\n",
    " \\\\ \\cos m \\theta_{d/2-1}\n",
    " \\\\ \\sin m \\theta_{d/2-1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "更深一步，这样的位置编码可以抽象为一种$\\beta$进制的编码过程。\n",
    "\n",
    "一个数 $n$ 在 $\\beta$ 进制下的第 $m$ 位：$x_m = \\left\\lfloor\\frac{n}{\\beta^{m-1}}\\right\\rfloor \\bmod \\beta$\n",
    "\n",
    "$$\\left[\\cos \\left(\\frac{n}{\\beta^{0}}\\right), \\sin \\left(\\frac{n}{\\beta^{0}}\\right), \\cos \\left(\\frac{n}{\\beta^{1}}\\right), \\sin \\left(\\frac{n}{\\beta^{1}}\\right), \\cdots, \\cos \\left(\\frac{n}{\\beta^{d / 2-1}}\\right), \\sin \\left(\\frac{n}{\\beta^{d / 2-1}}\\right)\\right]$$\n",
    "\n",
    "$$\\beta = 10000^{\\frac{2}{d}}$$\n",
    "\n",
    "取模本质上也是一个周期函数（和 $\\sin$,$\\cos$如出一辙。）因此，在不同位置的向量做编码，会携带的信息是位置 $n$ 在特定数位进制（$\\beta = 10000^{\\frac{2}{d}}$ 是一个超参数）下的表示。这样的进制编码信息本身就携带了相对位置信息和绝对位置信息。\n",
    "\n",
    "- sine embeddings\n",
    "\n",
    "- absolute embeddings\n",
    "\n",
    "- relative embeddings\n",
    "\n",
    "- rope embeddings\n",
    "\n",
    "**Relative positions encoding**:  $\\langle f(x, i), f(y, j) \\rangle = g(x, y, i - j)$\n",
    "\n",
    "Where $\\langle .,. \\rangle$ means the dot product(which is the attention score) for two different tokens. $g$ is a function which consider **relative positional embedding** as its input.\n",
    "\n",
    "For the original sime embeddings: $\\text{Embed}(x, i) = v_x + \\text{Positional Embedding}(\\text{pos})$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\langle \\text{Embed}(x, i), \\text{Embed}(y, j) \\rangle &= \\langle v_x + PE_i, v_y + PE_j \\rangle \\\\\n",
    "&= \\langle v_x, v_y \\rangle + \\langle v_x, PE_j \\rangle + \\langle PE_i, v_y \\rangle + \\langle PE_i, PE_j \\rangle\n",
    "\\end{aligned}$$\n",
    "\n",
    "$\\langle v_x, v_y \\rangle + \\langle v_x, PE_j \\rangle + \\dots$, these are cross-terms that are not relative.\n",
    "\n",
    "Thus, for the original traditional embeddings, in face of mountains of training data, the model tends to memorize the absolute position instead of the relative position, which would cause a rise of perplexity outside the boundary of training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326d23d",
   "metadata": {},
   "source": [
    "### RoPE\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "Unlike the simple adding operation ($v_x + \\text{PE}(i)$), RoPE tries to \"rotate\" the vector, which could perfectly satisfy the \"relative position\" property.\n",
    "\n",
    "Consider the most simple situation:\n",
    "\n",
    "$x = (x_0, x_1) \\in \\mathbb{R}^2$, we can compute $x'$ with a rotation matrix:\n",
    "\n",
    "$$R_{\\theta}=\\left(\\begin{array}{cc}\n",
    "\\cos \\theta & -\\sin \\theta \\\\\n",
    "\\sin \\theta & \\cos \\theta\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "$$x' = R_\\theta x$$\n",
    "\n",
    "The rotation of angles and doc product will only focus on the relative rotation!\n",
    "\n",
    "$$\\langle R_\\alpha a, R_\\beta b \\rangle = \\langle R_{(\\alpha - \\beta)} a, b \\rangle$$\n",
    "\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "Consider a word vector (we define it as $\\{q,k\\} = W_{\\{q, k\\}} x_m$), then we can apply the rotation matrix to the word vector in position $m$:\n",
    "\n",
    "$$\\{q, k\\}_m = R_{\\Theta, m}^d \\cdot \\{q, k\\}$$\n",
    "\n",
    "- Before the rotation, the vector does not contain any positional information. (only with semantic meaning)\n",
    "\n",
    "- After the rotation, the vector contain the positional information with the rotated angle. $\\theta_m$\n",
    "\n",
    "- The core component is the rotation matrix: $R_{\\Theta, m}^d$\n",
    "\n",
    "Of course, we know:\n",
    "\n",
    "$$R_{\\Theta, m}^2= \\begin{pmatrix} \\cos(m\\theta_j) & -\\sin(m\\theta_j) \\\\ \\sin(m\\theta_j) & \\cos(m\\theta_j) \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f738bb",
   "metadata": {},
   "source": [
    "> 下面的内容用中文，因为太精彩了。\n",
    "\n",
    "为了简化，我们在后续使用**复数运算**来代替对二维向量的运算（这两个是等价的），并且我们考虑 $d$ 是一个偶数，这样一个维度为 $d$ 的高维向量就可以被切割为 $\\frac{d}{2}$ 个二维向量。（对于 $q$ 和 $k$）\n",
    "\n",
    "令 $N = \\frac{d}{2}$，则原始的高维向量就被表示为了 $N$ 个复数表示：$\\{z_1, z_2, \\dots, z_j, \\dots, z_N\\}$, $z_j = x_{2j-1} + x_{2j} i$\n",
    "\n",
    "则旋转的过程可以表示为：\n",
    "\n",
    "$$z_j' = z_j \\cdot e^{im\\theta_j}$$\n",
    "\n",
    "$$\\theta_j = \\frac{1}{10000^{2(j-1)/d}}$$\n",
    "\n",
    "来一点数学：\n",
    "\n",
    "$$z_{j}^{\\prime}=z_{j} \\cdot e^{i m \\theta_{j}}=\\left(x_{2 j-1}+i x_{2 j}\\right)\\left(\\cos \\left(m \\theta_{j}\\right)+i \\sin \\left(m \\theta_{j}\\right)\\right)$$\n",
    "\n",
    "$$z_{j}^{\\prime}=\\left(x_{2 j-1} \\cos \\left(m \\theta_{j}\\right)-x_{2 j} \\sin \\left(m \\theta_{j}\\right)\\right)+i\\left(x_{2 j-1} \\sin \\left(m \\theta_{j}\\right)+x_{2 j} \\cos \\left(m \\theta_{j}\\right)\\right)$$\n",
    "\n",
    "其实，这就是一个二维向量 $(x_{2j-1}, x_{2j})$ 在施加旋转矩阵 $R_{\\Theta, m}^2$之后的结果。\n",
    "\n",
    "最终把向量的维度整合，$\\{q,k\\}_m$ 的结果就是对 $N$ 个二维向量分别施加旋转矩阵之后的结果，这些二维矩阵因为 $j = 1,2,\\dots, N$ 的选择取值不同因此带来旋转的角度也不同。\n",
    "\n",
    "当然，实际过程中不会具体地做高维向量的切割和合并，而是整合成一个大的分块矩阵：\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d = \n",
    "\\begin{pmatrix}\n",
    "    \\cos(m\\theta_1) & -\\sin(m\\theta_1) & 0 & 0 & \\dots \\\\\n",
    "    \\sin(m\\theta_1) & \\cos(m\\theta_1) & 0 & 0 & \\dots \\\\\n",
    "    0 & 0 & \\cos(m\\theta_2) & -\\sin(m\\theta_2) & \\dots \\\\\n",
    "    0 & 0 & \\sin(m\\theta_2) & \\cos(m\\theta_2) & \\dots \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{d \\times d}\n",
    "$$\n",
    "\n",
    "在施加旋转位置编码之后的高维向量就携带着相对位置信息进入 Attention 模块进行参数学习。\n",
    "\n",
    "#### Proof\n",
    "\n",
    "RoPE 的精妙之处在于，经过旋转操作后的两个向量 $\\mathbf{q}$ 和 $\\mathbf{k}$，它们的内积只依赖于它们之间的相对位置 $m-n$。\n",
    "    \n",
    "在注意力机制中，查询向量 $\\mathbf{q}$ 和键向量 $\\mathbf{k}$ 分别对应位置 $m$ 和 $n$。RoPE 对它们进行旋转操作：\n",
    "* 旋转后的查询向量：$\\mathbf{q}_m = \\mathbf{R}_m \\mathbf{q}$\n",
    "* 旋转后的键向量：$\\mathbf{k}_n = \\mathbf{R}_n \\mathbf{k}$\n",
    "\n",
    "它们之间的内积为：\n",
    "$$\n",
    "\\langle \\mathbf{q}_m, \\mathbf{k}_n \\rangle = \\langle \\mathbf{R}_m \\mathbf{q}, \\mathbf{R}_n \\mathbf{k} \\rangle\n",
    "$$\n",
    "利用旋转矩阵的性质，即 $\\mathbf{R}_m^T = \\mathbf{R}_{-m}$，以及 $\\mathbf{R}_m^T \\mathbf{R}_n = \\mathbf{R}_{n-m}$，我们可以得到：\n",
    "$$\n",
    "\\langle \\mathbf{R}_m \\mathbf{q}, \\mathbf{R}_n \\mathbf{k} \\rangle = \\langle \\mathbf{q}, \\mathbf{R}_m^T \\mathbf{R}_n \\mathbf{k} \\rangle = \\langle \\mathbf{q}, \\mathbf{R}_{n-m} \\mathbf{k} \\rangle\n",
    "$$\n",
    "这个结果表明，旋转后向量的内积（即注意力分数）只取决于相对位置 $n-m$，而不是绝对位置 $m$ 和 $n$。这正是 RoPE 能够有效编码相对位置信息的关键。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394daa91",
   "metadata": {},
   "source": [
    "#### More Than That...\n",
    "\n",
    "相关资料参考：\n",
    "\n",
    "- [Paper: Roformer](https://arxiv.org/abs/2104.09864)\n",
    "\n",
    "- [Transformer 升级之路之 RoPE 编码](https://spaces.ac.cn/archives/9675)\n",
    "\n",
    "- [Transformer 升级之路之 位置编码](https://spaces.ac.cn/archives/8231)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1dc158",
   "metadata": {},
   "source": [
    "# Hyperparameter Settings"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
