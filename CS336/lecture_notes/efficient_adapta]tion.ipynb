{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fa8dc5",
   "metadata": {},
   "source": [
    "# CS224N Lecture 12: Evaluation & Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a297399",
   "metadata": {},
   "source": [
    "## Intro: We are on the second half\n",
    "\n",
    "Original article: [We are on the second half by Shunyu Yao](https://ysymyth.github.io/The-Second-Half/)\n",
    "\n",
    "> tldr: We are at AI's second half, benchmarking is much more important than training.\n",
    "\n",
    "In three words: **RL finally works**. More precisely: **RL finally generalizes**. After several major detours and a culmination of milestones, we’ve landed on a working recipe to solve a wide range of RL tasks using language and reasoning. Even a year ago, if you told most AI researchers that a single recipe could tackle software engineering, creative writing, IMO-level math, mouse-and-keyboard manipulation, and long-form question answering — they’d laugh at your hallucinations. Each of these tasks is incredibly difficult and many researchers spend their entire PhDs focused on just one narrow slice.\n",
    "\n",
    "Up to now, thanks to efficient \"pre-training - post-training\" paradigm, AI has successfully solved nearly all learnable problems with clear answers, even achieving performance far beyond human level. Thus, the bottleneck for AI development lies in:\n",
    "\n",
    "- How to maximize AI's potential and prevent it from being confined to a limited dialogue box: We have **Agents**.\n",
    "\n",
    "- How to leverage AI's generalization problems and solving more problems that have no accurate answer (creative & innovative)?\n",
    "\n",
    "Thus, the capability of foundation model still needs to be improved.\n",
    "\n",
    "In the past, we mainly focus on getting high-quality training data, training or tuning better models and finding innovative algorithms and methods. This leads to the great progress of LLM technical improvement. In this case, **Evaluation and Benchmarking** seems less important, for they are obvious on those training tasks. (Maths, Codes, Problem solving...)\n",
    "\n",
    "> In contrast, defining tasks for AI often felt more straightforward: we simply took tasks humans already do (like translation, image recognition, or chess) and turned them into benchmarks. Not much insight or even engineering.\n",
    "\n",
    "### Example: Reinforcement Learning\n",
    "\n",
    "- algorithms: attract more attention!\n",
    "- environment: show less attention, but of equal (in current status, even more) importance.\n",
    "    - e.g.: `gym` in OpenAI.\n",
    "- priors: 先验知识\n",
    "    - For the initial state, the model can only do exploration but no exploitation, which leads to extremely bad results.\n",
    "    - e.g.: Imitate Learning, pre-training models\n",
    "\n",
    "{% note primary %}\n",
    "\n",
    "Language pre-training created good priors for chatting, but not equally good for **controlling computers or playing video games**. Why? These domains are further from the distribution of Internet text, and naively doing SFT / RL on these domains generalizes poorly. I noticed the problem in 2019, when GPT-2 just came out and I did SFT / RL on top of it to solve text-based games - CALM was the first agent in the world built via pre-trained language models. But it took millions of RL steps for the agent to hillclimb a single game, and it doesn’t transfer to new games. Though that’s exactly the characteristic of RL and nothing strange to RL researchers, I found it weird because we humans can easily play a new game and be significantly better zero-shot. Then I hit one of the first eureka moment in my life - we generalize because we can choose to do more than “go to cabinet 2” or “open chest 3 with key 1” or “kill dungeon with sword”, we can also choose to think about things like “The dungeon is dangerous and I need a weapon to fight with it. There is no visible weapon so maybe I need to find one in locked boxes or chests. Chest 3 is in Cabinet 2, let me first go there and unlock it”.\n",
    "\n",
    "\n",
    "> We still need to collect high-quality data in the pre-training process, which agents can learn general abilities.\n",
    "\n",
    "{% endnote %}\n",
    "\n",
    "### Tradition RL & Reasoning\n",
    "\n",
    "For the reasoning process, the tradition RL view may be quite strange, for its action space is infinitely (almost) large, which could cause expectations from sampling will be zero. (I think that is why strong-reasoning models may act hallucination in real-world actions.)\n",
    "\n",
    "However, the reasoning will lead to augmentation of generalization, choosing these boxes prepare you to better choose the box with money for any given game. My abstract explanation would be: **language generalizes through reasoning in agents**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48460c7",
   "metadata": {},
   "source": [
    "### The second half\n",
    "\n",
    "- Current benchmark has been explored, for the great enhancement of generalization ability for SOTA foundation models.\n",
    "\n",
    "{% note primary %}\n",
    "\n",
    "If novel methods are no longer needed and harder benchmarks will just get solved increasingly soon, what should we do?\n",
    "\n",
    "{% endnote %}\n",
    "\n",
    "Evaluation process should be refactored and rebuilt. **It means not just to create new and harder benchmarks, but to fundamentally question existing evaluation setups and create new ones**, so that we are forced to invent new methods beyond the working recipe. It is hard because humans have inertia(惯性) and seldom question basic assumptions - you just take them for granted without realizing they are assumptions, not laws: the utility problems.\n",
    "\n",
    "- Agents should not pass the benchmark automatically. In real-world problems, agent should frequently interacting with humans.\n",
    "\n",
    "- Agents should not run i.i.d (independent and identically distributed). Real world tasks should be run mostly sequentially.\n",
    "    - Long-term Memory Methods & and some benchmarks!\n",
    "\n",
    "\n",
    "{% note primary %}\n",
    "\n",
    "This game is hard because it is unfamiliar. But it is exciting. While players in the first half solve video games and exams, players in the second half get to build billion or trillion dollar companies by building useful products out of intelligence. While the first half is filled with incremental methods and models, the second half filters them to some degree. The general recipe would just crush your incremental methods, unless you create new assumptions that break the recipe. Then you get to do truly game-changing research.\n",
    "\n",
    "\n",
    "\n",
    "{% endnote %}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
