{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fa8dc5",
   "metadata": {},
   "source": [
    "# CS224N Lecture 12: Evaluation & Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a297399",
   "metadata": {},
   "source": [
    "## Intro: We are on the second half\n",
    "\n",
    "Original article: [We are on the second half by Shunyu Yao](https://ysymyth.github.io/The-Second-Half/)\n",
    "\n",
    "> tldr: We are at AI's second half, benchmarking is much more important than training.\n",
    "\n",
    "In three words: **RL finally works**. More precisely: **RL finally generalizes**. After several major detours and a culmination of milestones, we’ve landed on a working recipe to solve a wide range of RL tasks using language and reasoning. Even a year ago, if you told most AI researchers that a single recipe could tackle software engineering, creative writing, IMO-level math, mouse-and-keyboard manipulation, and long-form question answering — they’d laugh at your hallucinations. Each of these tasks is incredibly difficult and many researchers spend their entire PhDs focused on just one narrow slice.\n",
    "\n",
    "Up to now, thanks to efficient \"pre-training - post-training\" paradigm, AI has successfully solved nearly all learnable problems with clear answers, even achieving performance far beyond human level. Thus, the bottleneck for AI development lies in:\n",
    "\n",
    "- How to maximize AI's potential and prevent it from being confined to a limited dialogue box: We have **Agents**.\n",
    "\n",
    "- How to leverage AI's generalization problems and solving more problems that have no accurate answer (creative & innovative)?\n",
    "\n",
    "Thus, the capability of foundation model still needs to be improved.\n",
    "\n",
    "In the past, we mainly focus on getting high-quality training data, training or tuning better models and finding innovative algorithms and methods. This leads to the great progress of LLM technical improvement. In this case, **Evaluation and Benchmarking** seems less important, for they are obvious on those training tasks. (Maths, Codes, Problem solving...)\n",
    "\n",
    "> In contrast, defining tasks for AI often felt more straightforward: we simply took tasks humans already do (like translation, image recognition, or chess) and turned them into benchmarks. Not much insight or even engineering.\n",
    "\n",
    "### Example: Reinforcement Learning\n",
    "\n",
    "- algorithms: attract more attention!\n",
    "- environment: show less attention, but of equal (in current status, even more) importance.\n",
    "    - e.g.: `gym` in OpenAI.\n",
    "- priors: 先验知识\n",
    "    - For the initial state, the model can only do exploration but no exploitation, which leads to extremely bad results.\n",
    "    - e.g.: Imitate Learning, pre-training models\n",
    "\n",
    "{% note primary %}\n",
    "\n",
    "Language pre-training created good priors for chatting, but not equally good for **controlling computers or playing video games**. Why? These domains are further from the distribution of Internet text, and naively doing SFT / RL on these domains generalizes poorly. I noticed the problem in 2019, when GPT-2 just came out and I did SFT / RL on top of it to solve text-based games - CALM was the first agent in the world built via pre-trained language models. But it took millions of RL steps for the agent to hillclimb a single game, and it doesn’t transfer to new games. Though that’s exactly the characteristic of RL and nothing strange to RL researchers, I found it weird because we humans can easily play a new game and be significantly better zero-shot. Then I hit one of the first eureka moment in my life - we generalize because we can choose to do more than “go to cabinet 2” or “open chest 3 with key 1” or “kill dungeon with sword”, we can also choose to think about things like “The dungeon is dangerous and I need a weapon to fight with it. There is no visible weapon so maybe I need to find one in locked boxes or chests. Chest 3 is in Cabinet 2, let me first go there and unlock it”.\n",
    "\n",
    "\n",
    "> We still need to collect high-quality data in the pre-training process, which agents can learn general abilities.\n",
    "\n",
    "{% endnote %}\n",
    "\n",
    "### Tradition RL & Reasoning\n",
    "\n",
    "For the reasoning process, the tradition RL view may be quite strange, for its action space is infinitely (almost) large, which could cause expectations from sampling will be zero. (I think that is why strong-reasoning models may act hallucination in real-world actions.)\n",
    "\n",
    "However, the reasoning will lead to augmentation of generalization, choosing these boxes prepare you to better choose the box with money for any given game. My abstract explanation would be: **language generalizes through reasoning in agents**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48460c7",
   "metadata": {},
   "source": [
    "### The second half\n",
    "\n",
    "- Current benchmark has been explored, for the great enhancement of generalization ability for SOTA foundation models.\n",
    "\n",
    "{% note primary %}\n",
    "\n",
    "If novel methods are no longer needed and harder benchmarks will just get solved increasingly soon, what should we do?\n",
    "\n",
    "{% endnote %}\n",
    "\n",
    "Evaluation process should be refactored and rebuilt. **It means not just to create new and harder benchmarks, but to fundamentally question existing evaluation setups and create new ones**, so that we are forced to invent new methods beyond the working recipe. It is hard because humans have inertia(惯性) and seldom question basic assumptions - you just take them for granted without realizing they are assumptions, not laws: the utility problems.\n",
    "\n",
    "- Agents should not pass the benchmark automatically. In real-world problems, agent should frequently interacting with humans.\n",
    "\n",
    "- Agents should not run i.i.d (independent and identically distributed). Real world tasks should be run mostly sequentially.\n",
    "    - Long-term Memory Methods & and some benchmarks!\n",
    "\n",
    "\n",
    "{% note primary %}\n",
    "\n",
    "This game is hard because it is unfamiliar. But it is exciting. While players in the first half solve video games and exams, players in the second half get to build billion or trillion dollar companies by building useful products out of intelligence. While the first half is filled with incremental methods and models, the second half filters them to some degree. The general recipe would just crush your incremental methods, unless you create new assumptions that break the recipe. Then you get to do truly game-changing research.\n",
    "\n",
    "\n",
    "\n",
    "{% endnote %}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4481d",
   "metadata": {},
   "source": [
    "## Types of Evaluation\n",
    "\n",
    "- Close-ended\n",
    "    - Has fixed answers, can be grades easily.\n",
    "    - Several Metrics: F1 score, precision, recall rate, average accuracy, AUC score.\n",
    "    - Methods to aggregate different metrics? **A complex task** for different benchmark has different meanings! \n",
    "- Open-ended\n",
    "\n",
    "## Close Ended Evaluations\n",
    "\n",
    "Achieving accurate results doesn't mean this test set accurately measures the performance of large language models. A significant number of inaccurate and fabricated tests have been discovered by relevant research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef9942",
   "metadata": {},
   "source": [
    "### Example: SNLI (Stanford Natural Language Inference)\n",
    "\n",
    "SNLI stands for **Stanford Natural Language Inference**. It's a large, widely-used dataset specifically designed for the task of **Natural Language Inference (NLI)**.\n",
    "\n",
    "Natural Language Inference is a task where you determine the logical relationship between a pair of sentences: a **premise** and a **hypothesis**. Your goal is to classify this relationship into one of three categories:\n",
    "\n",
    "1.  **Entailment:** If the premise is true, the hypothesis must also be true.\n",
    "    * **Premise:** A man is riding a horse.\n",
    "    * **Hypothesis:** A man is outdoors.\n",
    "    * **Relationship:** Entailment (Because if he's riding a horse, he's very likely outdoors).\n",
    "\n",
    "2.  **Contradiction:** If the premise is true, the hypothesis must be false.\n",
    "    * **Premise:** A cat is sleeping on the mat.\n",
    "    * **Hypothesis:** A dog is standing awake.\n",
    "    * **Relationship:** Contradiction (These two sentences describe conflicting situations).\n",
    "\n",
    "3.  **Neutral:** There is no clear logical relationship between the premise and the hypothesis.\n",
    "    * **Premise:** Several women are talking about an exhibit.\n",
    "    * **Hypothesis:** They are looking at paintings.\n",
    "    * **Relationship:** Neutral (They might be looking at paintings, but we can't be certain from the premise alone).\n",
    "\n",
    "The SNLI dataset contains over **570,000** premise-hypothesis pairs, each manually labeled with one of the three relationships. The premise sentences are sourced from the **Flickr 30k** dataset, which are sentences that describe images. The hypothesis sentences were created by human crowdworkers based on those premises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805998be",
   "metadata": {},
   "source": [
    "## Open Ended Evaluations\n",
    "\n",
    "For long generation tasks (e.g. Translating, Summary...), the grading and judges of the given tests are continuous, these are open ended evaluations.\n",
    "\n",
    "- Content overlap metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9db810",
   "metadata": {},
   "source": [
    "### Content overlap metrics\n",
    "\n",
    "- Using **Word overlap–based metrics (BLEU, ROUGE, METEOR, CIDEr, etc.)**.\n",
    "\n",
    "    - Fit and efficient for simple translating and summary tasks.\n",
    "\n",
    "    - Using N-grams for evaluate similarity beyond the word layer. (BLEU: Bilingual Evaluation Understudy)\n",
    "\n",
    "- **Model-based metrics** to capture more semantics\n",
    "    - Use learned representations of words and sentences to compute semantic similarity between generated and reference texts.\n",
    "\n",
    "    - No more N-grams bottle necks for the trained bottleneck.\n",
    "\n",
    "    - The embeddings are pretrained, distance metrics used to measure the similarity can be fixed.\n",
    "\n",
    "    - Vector Similarity, Word Mover's Distance, BERTSCORE.\n",
    "\n",
    "- Beyond simple model-based metrics\n",
    "    - Sentence Movers Similarity: Based on **Word Movers Distance** to evaluate text in a continuous space using sentence embeddings from recurrent neural network representations.\n",
    "    - BLEURT: A regression model based on BERT returns a score that indicates to what extent the candidate text is **grammatical and conveys the meaning of the reference text**.\n",
    "        - Very important thought: pre-training large language models (or just simply using large language models) to evaluate for grasping more semantic meaning.\n",
    "\n",
    "> From BLEU to nowadays, the evaluation has never stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d43cd",
   "metadata": {},
   "source": [
    "## Human Evaluations\n",
    "\n",
    "Original and naive grading from human feedback is weak and biased!\n",
    "\n",
    "- Results are inconsistent / not reproducible\n",
    "- can be illogical\n",
    "- misinterpret your question\n",
    "- Precision not recall.\n",
    "\n",
    "For human evaluation, we often use **Arena** (Just like RLHF in post-training process).\n",
    "- Side by side ratings will eliminate the bias.\n",
    "\n",
    "As large language models shows great generalization capabilities and context-learning abilities, we can use LLM as the judger to replace human.\n",
    "> Compared with humans, LLM has less bias and more consistency.\n",
    "\n",
    "\n",
    "### AlpacaEval\n",
    "\n",
    "See the paper and repo, quite important work!\n",
    "\n",
    "- [GitHub Repo](https://github.com/tatsu-lab/alpaca_eval)\n",
    "- [Paper](https://arxiv.org/abs/2404.04475)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d95470",
   "metadata": {},
   "source": [
    "## Current Challenges\n",
    "\n",
    "- Consistency (Hard to know if we are evaluating the right thing)\n",
    "\n",
    "- Contaminations (Data contaminations from the pre-training data)\n",
    "\n",
    "- Biases"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
