{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa48fb05",
   "metadata": {},
   "source": [
    "# Lecture 3 Architectures & Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bdddd4",
   "metadata": {},
   "source": [
    "# Architecture for Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3ee50f",
   "metadata": {},
   "source": [
    "Starting with the original transformer...\n",
    "\n",
    "- (Masked) MultiHead Attention\n",
    "\n",
    "- Positional Embeddings\n",
    "\n",
    "- FFN (MLP), with RELU activation function.\n",
    "\n",
    "- LayerNorm\n",
    "\n",
    "However, the original transformer is designed for machine translation, which are viewed as a variant of RNNs. Nowadays, different models have different architecture design in detail:\n",
    "\n",
    "| Name | Year | LayerNorm | Parallel Layer | Pre-norm | Position embedding | Activations | Stability tricks |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Original transformer | 2017 | LayerNorm | Serial | ❌ | Sine | ReLU | |\n",
    "| GPT | 2018 | LayerNorm | Serial | ❌ | Absolute | GeLU | |\n",
    "| T5 (11B) | 2019 | RMSNorm | Serial | ✅ | Relative | ReLU | |\n",
    "| GPT2 | 2019 | LayerNorm | Serial | ❌ | Absolute | GeLU | |\n",
    "| GPT2 | 2020 | RMSNorm | Serial | ✅ | Relative | GeGLU | |\n",
    "| T5 (XXL 11B) v1.1 | 2020 | RMSNorm | Serial | ✅ | Relative | GeGLU | |\n",
    "| mT5 | 2020 | RMSNorm | Serial | ✅ | Relative | GeGLU | |\n",
    "| GPT3 (175B) | 2020 | LayerNorm | Serial | ✅ | Absolute | GeLU | |\n",
    "| GPTJ | 2021 | LayerNorm | Parallel | ✅ | RoPE | GeLU | |\n",
    "| LaMDA | 2021 | | | | Relative | GeGLU | |\n",
    "| Anthropic LM (not claude) | 2021 | | | | | | |\n",
    "| Gopher (280B) | 2021 | RMSNorm | Serial | ✅ | Relative | ReLU | |\n",
    "| GPT-NeoX | 2022 | LayerNorm | Parallel | ✅ | RoPE | GeLU | |\n",
    "| BLOOM (175B) | 2022 | LayerNorm | Parallel | ✅ | Alibi | GeLU | |\n",
    "| OPT (175B) | 2022 | LayerNorm | Serial | ❌ | Absolute | ReLU | |\n",
    "| PaLM (540B) | 2022 | RMSNorm | Parallel | ✅ | RoPE | SwiGLU | Z-loss |\n",
    "| Chinchilla | 2022 | RMSNorm | Serial | ✅ | Relative | ReLU | |\n",
    "| Mistral (7B) | 2023 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| LLaMA2 (70B) | 2023 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| LLaMA (65B) | 2023 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| GPT4 | 2023 | | | ❌ | | | |\n",
    "| Olmo 2 | 2024 | RMSNorm | Serial | ❌ | RoPE | SwiGLU | Z-loss, QK-norm |\n",
    "| Gemma 2 (27B) | 2024 | RMSNorm | Serial | ✅ | RoPE | GeGLU | Logit soft capping, Pre+post norm |\n",
    "| Nemotron-4 (340B) | 2024 | LayerNorm | Serial | ✅ | RoPE | SqReLu | |\n",
    "| Qwen 2 (72B) - same for 2.5 | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Falcon 2 11B | 2024 | LayerNorm | Parallel | ✅ | RoPE | GeLU | Z-loss |\n",
    "| Ph3 (small) - same for ph4 | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Llama 3 (70B) | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Reka Flash | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Command R+ | 2024 | LayerNorm | Parallel | ✅ | RoPE | SwiGLU | |\n",
    "| OLMo | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Qwen (14B) | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| DeepSeek (67B) | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Yi (34B) | 2024 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |\n",
    "| Mixtral of Experts | 2024 | | | ❌ | | | |\n",
    "| Command A | 2025 | LayerNorm | Parallel | ✅ | Hybrid (RoPE+NoPE) | SwiGLU | |\n",
    "| Gemma 3 | 2025 | RMSNorm | Serial | ✅ | RoPE | GeGLU | Pre+post norm, QK-norm |\n",
    "| SmolLM2 (1.7B) | 2025 | RMSNorm | Serial | ✅ | RoPE | SwiGLU | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad97fdf",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1d929",
   "metadata": {},
   "source": [
    "### PreNorm vs PostNorm\n",
    "\n",
    "![Norm](../../assets/imgs/norm.png)\n",
    "\n",
    "Several discussions for PreNorm and PostNorm:\n",
    "\n",
    "- [Discussion](https://kexue.fm/archives/9009)\n",
    "\n",
    "**TLDR**: With the same parameter settings, pre-norm can be trained more easily than post-norm, however, the final performance does not outweigh post-norm.(or exactly the same for the pre-training process.)\n",
    "\n",
    "However, for the post-training fine-tuning, the post-normalization architecture has better transfer performance.\n",
    "\n",
    "> However, the gradients of Pre-LN at bottom layers tend to be larger than at top layers, leading to a degradation in performance compared with Post-LN.\n",
    "\n",
    "But for pre-norm, which can keep the good parts of residual connections, with nicer gradient propagation and fewer spike."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2eb4e",
   "metadata": {},
   "source": [
    "Maybe we can do Double Norm ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a95306",
   "metadata": {},
   "source": [
    "### Layer Norm & RMS Norm\n",
    "\n",
    "#### Layer Normalization\n",
    "\n",
    "在层归一化中，我们沿着 **特征维度 ($D$)** 计算统计量，对每个 **样本 ($N$)** 和 **序列位置 ($L$)** 进行归一化。\n",
    "这意味着 $\\mu$ 和 $\\sigma^2$ 是针对每个样本 $n$ 和每个序列位置 $l$ 计算的。\n",
    "\n",
    "* **均值 $\\mu_{n,l}$：**\n",
    "    $$\\mu_{n,l} = \\frac{1}{D} \\sum_{d=1}^D x_{n,l,d}$$\n",
    "* **方差 $\\sigma_{n,l}^2$：**\n",
    "    $$\\sigma_{n,l}^2 = \\frac{1}{D} \\sum_{d=1}^D (x_{n,l,d} - \\mu_{n,l})^2$$\n",
    "* **缩放因子和平移因子：** $\\gamma$ 和 $\\beta$ 在所有样本和序列位置上共享，但通常也是与特征维度 $D$ 相同长度的向量，即 $\\gamma, \\beta \\in \\mathbb{R}^{D}$。\n",
    "    因此，对于每个 $x_{n,l,d}$：\n",
    "    $$y_{n,l,d} = \\gamma_d \\left( \\frac{x_{n,l,d} - \\mu_{n,l}}{\\sqrt{\\sigma_{n,l}^2 + \\epsilon}} \\right) + \\beta_d$$\n",
    "\n",
    "> 在 Transformer 等序列建模模型中，因为序列长度 $N$ 可变（并且是频繁变化），因此为了提升模型的鲁棒性，往往采用 **Layer Normalization** 的方式实现归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d131709",
   "metadata": {},
   "source": [
    "#### RMS Norm\n",
    "\n",
    "RMSNorm, which stands for **Root Mean Square Normalization**, is a layer normalization technique used in some transformer architectures. It's a simpler and more computationally efficient alternative to traditional Layer Normalization.\n",
    "\n",
    "The core principle of RMSNorm is to **re-scale the activations of a layer based on their root mean square (RMS)** value, rather than their mean and standard deviation.\n",
    "\n",
    "For a given input vector $x$ (the activations from a layer), the RMSNorm operation is defined as:\n",
    "\n",
    "> The normalization also happens in the dimensional feature.\n",
    "\n",
    "$$y_{n,l,d} = \\gamma_d \\left( \\frac{x_{n,l,d}}{\\sqrt{\\frac{1}{D} \\sum_{i=1}^{D} x_{n,l,i}^2 + \\epsilon}} \\right)$$\n",
    "\n",
    "\n",
    "**RMSNorm** calculates the root mean square, which is essentially the L2 norm divided by the square root of the dimension. This method is simpler as it **doesn't subtract the mean**, which is the most computationally expensive part of Layer Normalization. This makes RMSNorm more **efficient and faster**, especially in large-scale models (Runtime: Memory Movement). It normalizes the magnitude of the activations, which is often sufficient for stabilizing training.\n",
    "\n",
    "- also: it drops bias terms for memory and optimization stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213a2fd",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "For non-linear components in the neural network.\n",
    "\n",
    "### GLU (Gated Activations)\n",
    "\n",
    "Original Feed Forward Network: $FF(x) = \\max(0, xW_1)W_2$.\n",
    "\n",
    "With GLU:\n",
    "\n",
    "$$\\max(0, xW_1) \\to \\max(0, xW_1) \\otimes (xV)$$\n",
    "\n",
    "$\\otimes$ means element-wise operations, the former part of GLU is also the RELU function, and the latter let the input $x$ passing through the matrix $V$ and controls which message can be passed into this successfully.\n",
    "\n",
    "$$\\text{FF}_{\\text{REGLU}}(x) = (\\max(0, xW_1) \\otimes (xV) ) W_2$$\n",
    "\n",
    "### SwiGLU Mathematical Formula\n",
    "\n",
    "SwiGLU, which stands for **Swish-Gated Linear Unit**, is a variant of the Gated Linear Unit (GLU) family. It was introduced by Google Brain in the PaLM model and has since been adopted by other modern large language models, such as LLaMA, as the activation function within their feed-forward networks (FFN). It has been shown to outperform both ReLU and GeLU in terms of performance.\n",
    "\n",
    "SwiGLU functions as a sub-module within the feed-forward network. Its core idea is to **replace the traditional single-path activation with a two-path structure controlled by a \"gate.\"**.\n",
    "\n",
    "**SwiGLU-based FFN**:\n",
    "$$FFN(x) = \\text{SwiGLU}(xW_1, xW_2)W_3$$\n",
    "\n",
    "Here, $x$ is the input vector, and $W_1, W_2, W_3$ are learnable weight matrices.\n",
    "\n",
    "The core operation of SwiGLU is defined as:\n",
    "\n",
    "$$\\text{SwiGLU}(x, W_1, W_2) = (\\text{Swish}(xW_1)) \\otimes (xW_2)$$\n",
    "\n",
    "Where:\n",
    "* $x$ is the input vector.\n",
    "* $W_1$ and $W_2$ are two distinct weight matrices.\n",
    "* **$\\text{Swish}(\\cdot)$** is the activation function defined by:\n",
    "  $$\\text{Swish}(z) = z \\cdot \\sigma(z)$$\n",
    "  where $\\sigma(z)$ is the Sigmoid function:\n",
    "  $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "* **$\\otimes$** denotes **element-wise multiplication**.\n",
    "\n",
    "1.  **Path 1 (Activation Path)**: The input vector $x$ is multiplied by the weight matrix $W_1$ to produce a vector $z_1 = xW_1$. This vector is then passed through the Swish activation function: Swish($z_1$).\n",
    "2.  **Path 2 (Gating Path)**: The input vector $x$ is multiplied by a separate weight matrix $W_2$ to produce another vector $z_2 = xW_2$.\n",
    "3.  **Gating**: The result from Path 1, Swish($z_1$), is multiplied element-wise by the result from Path 2,$z_2$.\n",
    "\n",
    "This element-wise multiplication is the key \"gating\" mechanism. The second path, $z_2 = xW_2$, acts as a gate, dynamically adjusting the weight of each element in the activated vector from Path 1 based on the input $x$.\n",
    "\n",
    "* **Dynamic Information Flow**: The gating design allows the network to learn which features should be amplified or suppressed, providing a more precise way to control information flow. This adaptive gating ability helps the model better capture complex dependencies.\n",
    "* **Smoother Gradients**: Unlike ReLU's hard cutoff (with a gradient of 0 or 1), the Swish function has a smooth gradient. This helps in mitigating the vanishing gradient problem in deep networks, leading to more stable optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f19641",
   "metadata": {},
   "source": [
    "## Serial vs Parallel layers\n",
    "\n",
    "Original serialized attention mechanism cannot do parallel computations.\n",
    "\n",
    "- $y = x + \\text{MLP}(\\text{LayerNorm}(x + \\text{Attention}(\\text{LayerNorm}(x))))$\n",
    "\n",
    "- $y = x + \\text{MLP}(\\text{LayerNorm}(x)) + \\text{Attention}(\\text{LayerNorm}(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecfbeca",
   "metadata": {},
   "source": [
    "## Positional Embeddings\n",
    "\n",
    "- sine embeddings\n",
    "\n",
    "- absolute embeddings\n",
    "\n",
    "- relative embeddings\n",
    "\n",
    "- rope embeddings\n",
    "\n",
    "**Relative positions encoding**:  $\\langle f(x, i), f(y, j) \\rangle = g(x, y, i - j)$\n",
    "\n",
    "Where $\\langle .,. \\rangle$ means the dot product(which is the attention score) for two different tokens. $g$ is a function which consider **relative positional embedding** as its input.\n",
    "\n",
    "For the original sime embeddings: $\\text{Embed}(x, i) = v_x + \\text{Positional Embedding}(\\text{pos})$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\langle \\text{Embed}(x, i), \\text{Embed}(y, j) \\rangle &= \\langle v_x + PE_i, v_y + PE_j \\rangle \\\\\n",
    "&= \\langle v_x, v_y \\rangle + \\langle v_x, PE_j \\rangle + \\langle PE_i, v_y \\rangle + \\langle PE_i, PE_j \\rangle\n",
    "\\end{aligned}$$\n",
    "\n",
    "$\\langle v_x, v_y \\rangle + \\langle v_x, PE_j \\rangle + \\dots$, these are cross-terms that are not relative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326d23d",
   "metadata": {},
   "source": [
    "### RoPE\n",
    "\n",
    "Consider the most simple situation:\n",
    "\n",
    "$x = (x_0, x_1) \\in \\mathbb{R}^2$, we can compute $x'$ with a rotation matrix:\n",
    "\n",
    "$$R_{\\theta}=\\left(\\begin{array}{cc}\n",
    "\\cos \\theta & -\\sin \\theta \\\\\n",
    "\\sin \\theta & \\cos \\theta\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "$$x' = R_\\theta x$$\n",
    "\n",
    "The rotation of angles and doc product will only focus on the relative rotation!\n",
    "\n",
    "$$\\langle R_\\alpha a, R_\\beta b \\rangle = \\langle R_{(\\alpha - \\beta)} a, b \\rangle$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1dc158",
   "metadata": {},
   "source": [
    "# Hyperparameter Settings"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
