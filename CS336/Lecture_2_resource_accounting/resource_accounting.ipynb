{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16abfe02",
   "metadata": {},
   "source": [
    "# Lecture 2: Pytorch & Resource Accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9257cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40ab85",
   "metadata": {},
   "source": [
    "## Basic Concepts\n",
    "\n",
    "**Efficiency** matters!\n",
    "\n",
    "- Compute: FLOPS\n",
    "\n",
    "- Memory: GB\n",
    "\n",
    "\n",
    "Definition of FLOPS: a metric used to measure the computational power of a computer or processor. It indicates how many **floating-point operations** (calculations involving decimal numbers like addition, subtraction, multiplication, and division) a system can perform per second.\n",
    "\n",
    "$$ \\text{FLOPS (Ideal)} = \\text{Number of Cores} \\times \\text{Clock Frequency per Core} \\times \\text{Floating Point Operations per cycle} $$\n",
    "\n",
    "$$ \\text{FLOPS (Actual)} = \\frac{\\text{Total Number of Floating Point Operations Performed}}{\\text{Execution Times}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14622ab",
   "metadata": {},
   "source": [
    "## Memory Accounting\n",
    "\n",
    "Tensors are the basic building block for storing everything: parameters, gradients, optimizer state, data, activations.\n",
    "\n",
    "[Official Docs](https://docs.pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "Almost everything (parameters, gradients, activations, optimizer states) are stored as floating point numbers.\n",
    "\n",
    "How to compute memory bytes in Tensors?\n",
    "\n",
    "```python\n",
    "def get_memory_usage(x: torch.Tensor):\n",
    "    return x.numel() * x.element_size()\n",
    "\n",
    "# torch.numel: Returns the total number of elements in the input tensor.\n",
    "# torch.element_size: Returns the size in bytes of an individual element.\n",
    "```\n",
    "\n",
    "The result shows how many bytes (1 MB = $2^{20}$ bytes) a tensor is.\n",
    "\n",
    "### Basic Type\n",
    "\n",
    "- `float32`: 1 + 8 + 23, default type\n",
    "- `float16`: 1 + 5 + 10, cuts down the memory\n",
    "- `bfloat16`: 1 + 8 + 7.\n",
    "- `fp8`: 1 + 4 + 3 (FP8E4M3) & 1 + 5 + 2 (FP8E5M2)\n",
    "\n",
    "Google Brain developed bfloat (brain floating point) in 2018 to address this issue. bfloat16 uses the same memory as float16 but has the same dynamic range as float32! The only catch is that the resolution is worse, but this matters less for deep learning.\n",
    "\n",
    "[FP8](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html)\n",
    "\n",
    "Solution: **use mixed precision training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage(x: torch.Tensor):\n",
    "    return x.numel() * x.element_size()\n",
    "\n",
    "\n",
    "# torch.numel: Returns the total number of elements in the input tensor.\n",
    "# torch.element_size: Returns the size in bytes of an individual element.\n",
    "\n",
    "# for float 32\n",
    "x = torch.zeros((4, 8, 20))  # @inspect x\n",
    "print(x.dtype)\n",
    "print(\"Number of elements in this tensor: \", x.numel())\n",
    "print(\"The size of bytes for an individual element in this tensor: \", x.element_size())\n",
    "print(get_memory_usage(x), \"bytes\")\n",
    "print(get_memory_usage(x) / 2**20)\n",
    "\n",
    "# for empty tensor?\n",
    "try:\n",
    "    empty_tensor = torch.empty(4, 8)\n",
    "    print(get_memory_usage(empty_tensor))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# for float 16\n",
    "x = torch.ones((4, 8, 20), dtype=torch.float16)\n",
    "print(x.dtype)\n",
    "print(x.numel())\n",
    "print(x.element_size())\n",
    "# cut the half!\n",
    "print(get_memory_usage(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bytes_information(type):\n",
    "    x = torch.ones((4, 8, 20), dtype=type)\n",
    "    print(f\"=============={type}================\")\n",
    "    print(f\"Dtype: {x.dtype}\")\n",
    "    print(f\"Element size: {x.element_size()}\")\n",
    "    print(f\"Bytes: {get_memory_usage(x)}\")\n",
    "    print(f\"=============={type}================\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "TYPELIST = [torch.float64, torch.float32, torch.float, torch.float16, torch.bfloat16]\n",
    "\n",
    "for type in TYPELIST:\n",
    "    get_bytes_information(type=type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "float32_info = torch.finfo(torch.float32)  # @inspect float32_info\n",
    "float16_info = torch.finfo(torch.float16)  # @inspect float16_info\n",
    "bfloat16_info = torch.finfo(torch.bfloat16)  # @inspect bfloat16_info\n",
    "print(float16_info)\n",
    "print(float32_info)\n",
    "print(bfloat16_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa370ec5",
   "metadata": {},
   "source": [
    "## Compute Accounting\n",
    "\n",
    "By default, tensors are stored in CPU memory. However, in order to take advantage of the massive parallelism of GPUs, we need to move them to GPU memory.\n",
    "\n",
    "![GPU and CPU](https://stanford-cs336.github.io/spring2025-lectures/images/cpu-gpu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic information of GPUs\n",
    "num_gpus = torch.cuda.device_count()  # @inspect num_gpus\n",
    "for i in range(num_gpus):\n",
    "    properties = torch.cuda.get_device_properties(i)  # @inspect properties\n",
    "    print(properties)\n",
    "\n",
    "print(num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x = torch.zeros((4, 8, 10))\n",
    "print(x.device)\n",
    "\n",
    "# moving the cpu to gpu\n",
    "# quite slow if the tensor is large\n",
    "y = x.to(device=torch.device(\"cuda:0\"))\n",
    "\n",
    "\n",
    "def test_time_compute(x: torch.Tensor):\n",
    "    start_time = time.time()\n",
    "    moved = x.to(device=torch.device(\"cuda:0\"))\n",
    "    print(moved.device)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "\n",
    "\n",
    "test_time_compute(torch.zeros(size=(20, 20)))\n",
    "test_time_compute(torch.zeros(size=(50000, 50000)))\n",
    "\n",
    "\n",
    "# creating a tensor directly to gpu\n",
    "memory_allocated = torch.cuda.memory_allocated(\"cuda:1\")\n",
    "time_1 = time.time()\n",
    "z = torch.zeros(size=(50000, 50000), device=\"cuda:1\")\n",
    "time_2 = time.time()\n",
    "print(time_2 - time_1)\n",
    "memory_allocated_new = torch.cuda.memory_allocated(device=\"cuda:1\")\n",
    "memory_used = memory_allocated_new - memory_allocated\n",
    "\n",
    "print(f\"Memory Used: {memory_used}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
