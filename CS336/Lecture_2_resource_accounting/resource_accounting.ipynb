{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16abfe02",
   "metadata": {},
   "source": [
    "# Lecture 2: Pytorch & Resource Accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9257cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40ab85",
   "metadata": {},
   "source": [
    "## Basic Concepts\n",
    "\n",
    "**Efficiency** matters!\n",
    "\n",
    "- Compute: FLOPS\n",
    "\n",
    "- Memory: GB\n",
    "\n",
    "\n",
    "Definition of FLOPS: a metric used to measure the computational power of a computer or processor. It indicates how many **floating-point operations** (calculations involving decimal numbers like addition, subtraction, multiplication, and division) a system can perform per second.\n",
    "\n",
    "$$ \\text{FLOPS (Ideal)} = \\text{Number of Cores} \\times \\text{Clock Frequency per Core} \\times \\text{Floating Point Operations per cycle} $$\n",
    "\n",
    "$$ \\text{FLOPS (Actual)} = \\frac{\\text{Total Number of Floating Point Operations Performed}}{\\text{Execution Times}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14622ab",
   "metadata": {},
   "source": [
    "## Memory Accounting\n",
    "\n",
    "Tensors are the basic building block for storing everything: parameters, gradients, optimizer state, data, activations.\n",
    "\n",
    "[Official Docs](https://docs.pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "Almost everything (parameters, gradients, activations, optimizer states) are stored as floating point numbers.\n",
    "\n",
    "How to compute memory bytes in Tensors?\n",
    "\n",
    "```python\n",
    "def get_memory_usage(x: torch.Tensor):\n",
    "    return x.numel() * x.element_size()\n",
    "\n",
    "# torch.numel: Returns the total number of elements in the input tensor.\n",
    "# torch.element_size: Returns the size in bytes of an individual element.\n",
    "```\n",
    "\n",
    "The result shows how many bytes (1 MB = $2^{20}$ bytes) a tensor is.\n",
    "\n",
    "### Basic Type\n",
    "\n",
    "- `float32`: 1 + 8 + 23, default type\n",
    "- `float16`: 1 + 5 + 10, cuts down the memory\n",
    "- `bfloat16`: 1 + 8 + 7.\n",
    "- `fp8`: 1 + 4 + 3 (FP8E4M3) & 1 + 5 + 2 (FP8E5M2)\n",
    "\n",
    "Google Brain developed bfloat (brain floating point) in 2018 to address this issue. bfloat16 uses the same memory as float16 but has the same dynamic range as float32! The only catch is that the resolution is worse, but this matters less for deep learning.\n",
    "\n",
    "[FP8](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html)\n",
    "\n",
    "Solution: **use mixed precision training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage(x: torch.Tensor):\n",
    "    return x.numel() * x.element_size()\n",
    "\n",
    "\n",
    "# torch.numel: Returns the total number of elements in the input tensor.\n",
    "# torch.element_size: Returns the size in bytes of an individual element.\n",
    "\n",
    "# for float 32\n",
    "x = torch.zeros((4, 8, 20))  # @inspect x\n",
    "print(x.dtype)\n",
    "print(\"Number of elements in this tensor: \", x.numel())\n",
    "print(\"The size of bytes for an individual element in this tensor: \", x.element_size())\n",
    "print(get_memory_usage(x), \"bytes\")\n",
    "print(get_memory_usage(x) / 2**20)\n",
    "\n",
    "# for empty tensor?\n",
    "try:\n",
    "    empty_tensor = torch.empty(4, 8)\n",
    "    print(get_memory_usage(empty_tensor))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# for float 16\n",
    "x = torch.ones((4, 8, 20), dtype=torch.float16)\n",
    "print(x.dtype)\n",
    "print(x.numel())\n",
    "print(x.element_size())\n",
    "# cut the half!\n",
    "print(get_memory_usage(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bytes_information(type):\n",
    "    x = torch.ones((4, 8, 20), dtype=type)\n",
    "    print(f\"=============={type}================\")\n",
    "    print(f\"Dtype: {x.dtype}\")\n",
    "    print(f\"Element size: {x.element_size()}\")\n",
    "    print(f\"Bytes: {get_memory_usage(x)}\")\n",
    "    print(f\"=============={type}================\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "TYPELIST = [torch.float64, torch.float32, torch.float, torch.float16, torch.bfloat16]\n",
    "\n",
    "for type in TYPELIST:\n",
    "    get_bytes_information(type=type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "float32_info = torch.finfo(torch.float32)  # @inspect float32_info\n",
    "float16_info = torch.finfo(torch.float16)  # @inspect float16_info\n",
    "bfloat16_info = torch.finfo(torch.bfloat16)  # @inspect bfloat16_info\n",
    "print(float16_info)\n",
    "print(float32_info)\n",
    "print(bfloat16_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa370ec5",
   "metadata": {},
   "source": [
    "## Compute Accounting\n",
    "\n",
    "### Tensors on GPU\n",
    "\n",
    "By default, tensors are stored in CPU memory. However, in order to take advantage of the massive parallelism of GPUs, we need to move them to GPU memory.\n",
    "\n",
    "![GPU and CPU](https://stanford-cs336.github.io/spring2025-lectures/images/cpu-gpu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic information of GPUs\n",
    "num_gpus = torch.cuda.device_count()  # @inspect num_gpus\n",
    "for i in range(num_gpus):\n",
    "    properties = torch.cuda.get_device_properties(i)  # @inspect properties\n",
    "    print(properties)\n",
    "\n",
    "print(num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x = torch.zeros((4, 8, 10))\n",
    "print(x.device)\n",
    "\n",
    "# moving the cpu to gpu\n",
    "# quite slow if the tensor is large\n",
    "y = x.to(device=torch.device(\"cuda:0\"))\n",
    "\n",
    "\n",
    "def test_time_compute(x: torch.Tensor):\n",
    "    start_time = time.time()\n",
    "    moved = x.to(device=torch.device(\"cuda:0\"))\n",
    "    print(moved.device)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "\n",
    "\n",
    "test_time_compute(torch.zeros(size=(20, 20)))\n",
    "test_time_compute(torch.zeros(size=(50000, 50000)))\n",
    "\n",
    "\n",
    "# creating a tensor directly to gpu\n",
    "memory_allocated = torch.cuda.memory_allocated(\"cuda:1\")\n",
    "time_1 = time.time()\n",
    "z = torch.zeros(size=(50000, 50000), device=\"cuda:1\")\n",
    "time_2 = time.time()\n",
    "print(time_2 - time_1)\n",
    "memory_allocated_new = torch.cuda.memory_allocated(device=\"cuda:1\")\n",
    "memory_used = memory_allocated_new - memory_allocated\n",
    "\n",
    "print(f\"Memory Used: {memory_used}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31075f2",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "\n",
    "#### Tensor Storage\n",
    "\n",
    "PyTorch tensors are pointers into allocated memory, with metadata describing how to get to **any element** of the tensor.\n",
    "\n",
    "For the methods, we use [`.stride()`](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.stride.html)\n",
    "\n",
    "![Stride in torch](https://martinlwx.github.io/img/2D_tensor_strides.png)\n",
    "\n",
    "Stride is the jump necessary to go from one element to the next one in the specified dimension `dim`. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99587984",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.randint(1, 1000, size=(10, 10, 20))\n",
    "print(test_tensor.shape)\n",
    "print(test_tensor.dim())\n",
    "\n",
    "# for the first dimension, it will jump 200 steps for reaching the next element\n",
    "print(test_tensor.stride(0))\n",
    "\n",
    "# for the second dimension, it will jump 20 steps for reaching the next element\n",
    "print(test_tensor.stride(1))\n",
    "\n",
    "# for the last dimension, it will jump 1 step for reaching the next element\n",
    "print(test_tensor.stride(-1))\n",
    "\n",
    "print(test_tensor[2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90ba94",
   "metadata": {},
   "source": [
    "How it works? For example, I want to access the value of `test_tensor[i,j,k]`:\n",
    "I will move: `test_tensor.stride(0) * i + test_tensor.stride(1) * j + test_tensor.stride(2) * k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other operations for tensor: slicing & element_wise\n",
    "# ! all the elementwise operations are operated by single element!\n",
    "x = torch.Tensor([3,3,4])\n",
    "print(x.pow(2))\n",
    "print(x.rsqrt())\n",
    "\n",
    "# `triu` takes the upper triangular part of a matrix.\n",
    "test = torch.randint(1, 1000, size=(2, 2, 2))\n",
    "print(test.triu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9afba62",
   "metadata": {},
   "source": [
    "### Tensor Einops\n",
    "\n",
    "Einops is a library for manipulating tensors where dimensions are named. It is inspired by Einstein summation notation (Einstein, 1916).\n",
    "\n",
    "[Official Docs](https://einops.rocks/1-einops-basics/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# einops demo\n",
    "from einops import rearrange, reduce, repeat\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "# assume there is an image called background.png\n",
    "try:\n",
    "    pil_image = Image.open(\"../../img/background.png\")\n",
    "    original_tensor = transform(pil_image)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "    original_tensor = torch.randn(size=(4, 1144, 1718))\n",
    "\n",
    "original_tensor = original_tensor[:, :1140, :1700]\n",
    "print(original_tensor.shape)\n",
    "# torch.Size([4, 1144, 1718]): (c, h, w)\n",
    "\n",
    "\n",
    "def _to_img(my_tensor, file_path):\n",
    "    if my_tensor.max() > 1.0:\n",
    "        my_tensor = my_tensor / 255.0\n",
    "    to_pil_image = ToPILImage()\n",
    "    pil_image = to_pil_image(my_tensor)\n",
    "    pil_image.save(f\"../../img/{file_path}.png\")\n",
    "\n",
    "\n",
    "rearrange_tensor = rearrange(original_tensor, \"c h w -> c w h\")\n",
    "_to_img(rearrange_tensor, \"rearrange_background\")\n",
    "\n",
    "reduce_tensor = reduce(\n",
    "    original_tensor, \"c (h h2) (w w2) -> c h w\", \"mean\", h2=20, w2=20\n",
    ")\n",
    "# do average pooling (like the CNN) for given tensor\n",
    "print(reduce_tensor.shape)\n",
    "_to_img(reduce_tensor, \"reduce_background\")\n",
    "\n",
    "reduce_tensor = original_tensor[1, :, :].squeeze()\n",
    "print(reduce_tensor.shape)\n",
    "repeat_tensor = repeat(reduce_tensor, \"h w -> c h w\", c=4)\n",
    "_to_img(repeat_tensor, \"repeat_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934580d",
   "metadata": {},
   "source": [
    "#### JaxTyping\n",
    "\n",
    "`jaxtyping` is a Python library that provides type annotations for your array-based code, particularly for the JAX framework. Think of it as a tool that lets you add precise shape and data type information to your function signatures, going far beyond the basic `jax.Array` or `np.ndarray` type hints.\n",
    "\n",
    "For `torch.Tensor`, things get the same.\n",
    "\n",
    "Moreover, jax support JIT and auto-grad functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float\n",
    "x: Float[torch.Tensor, \"batch seq heads hidden\"] = torch.ones(2, 2, 1, 3)  # @inspect x\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61dbff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jaxtyping import Float, Int\n",
    "\n",
    "def matmul(\n",
    "    A: Float[jax.Array, \"batch_size in_features\"],\n",
    "    B: Float[jax.Array, \"in_features out_features\"]\n",
    ") -> Float[jax.Array, \"batch_size out_features\"]:\n",
    "    \"\"\"Performs matrix multiplication.\"\"\"\n",
    "    return A @ B\n",
    "\n",
    "# This will pass type checking\n",
    "A_good = jnp.zeros((128, 784))\n",
    "B_good = jnp.zeros((784, 10))\n",
    "result = matmul(A_good, B_good)\n",
    "print(result.shape) # (128, 10)\n",
    "\n",
    "# A static type checker will flag an error here because \"in_features\"\n",
    "# dimensions don't match (784 vs 600).\n",
    "A_bad = jnp.zeros((128, 784))\n",
    "B_bad = jnp.zeros((600, 10))\n",
    "# result = matmul(A_bad, B_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400745a1",
   "metadata": {},
   "source": [
    "#### `einsum`\n",
    "\n",
    "By using `einops`, we can run this code in a better way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83319311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float\n",
    "from einops import einsum\n",
    "\n",
    "B = 128\n",
    "SEQ1 = 100\n",
    "SEQ2 = 200\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "x: Float[torch.Tensor, \"batch seq1 hidden_dim\"] = torch.randn(size=(B, SEQ1, HIDDEN_DIM))\n",
    "y: Float[torch.Tensor, \"batch seq2 hidden_dim\"] = torch.randn(size=(B, SEQ2, HIDDEN_DIM))\n",
    "\n",
    "z = einsum(x, y, \"batch seq1 hidden_dim, batch seq2 hidden_dim -> batch seq1 seq2\")\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(torch.equal(z, (x @ y.transpose(-2, -1))))\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016eef52",
   "metadata": {},
   "source": [
    "#### `reduce`\n",
    "\n",
    "You can reduce a single tensor via some operation (e.g., sum, mean, max, min).\n",
    "\n",
    "```python\n",
    "x: Float[torch.Tensor, \"batch seq hidden\"] = torch.ones(2, 3, 4)  # @inspect x\n",
    "# Old way:\n",
    "y = x.mean(dim=-1)  # @inspect y\n",
    "# New (einops) way:\n",
    "y = reduce(x, \"... hidden -> ...\", \"sum\")  # @inspect y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "x: Float[torch.Tensor, \"batch seq hidden\"] = torch.randn(2, 3, 4)  # @inspect x\n",
    "\n",
    "# make the last dimension mean to 0\n",
    "x = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "\n",
    "y = reduce(x, \"... hidden -> ...\", \"sum\")  # @inspect y\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f421db7",
   "metadata": {},
   "source": [
    "#### `rearrange`\n",
    "\n",
    "Sometimes, a dimension represents two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x: Float[torch.Tensor, \"batch seq total_hidden\"] = torch.ones(2, 3, 8)  # @inspect x\n",
    "# ...where total_hidden is a flattened representation of heads * hidden1\n",
    "w: Float[torch.Tensor, \"hidden1 hidden2\"] = torch.ones(4, 2)\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "\n",
    "# Break up total_hidden into two dimensions (heads and hidden1):\n",
    "# total_hidden = hidden1 \\times hidden2\n",
    "x = rearrange(x, \"... (heads hidden1) -> ... heads hidden1\", heads=2)  # @inspect x\n",
    "print(f\"x shape: {x.shape}\")\n",
    "\n",
    "# Perform the transformation by w:\n",
    "x = einsum(x, w, \"... hidden1, hidden1 hidden2 -> ... hidden2\")  # @inspect x\n",
    "# Combine heads and hidden2 back together:\n",
    "print(f\"x shape: {x.shape}\")\n",
    "x = rearrange(x, \"... heads hidden2 -> ... (heads hidden2)\")  # @inspect x\n",
    "print(f\"x shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49ffce",
   "metadata": {},
   "source": [
    "### Computation Cost\n",
    "\n",
    "Having gone through all the operations, let us examine their computational cost.\n",
    "\n",
    "**A floating-point operation (FLOP)** is a basic operation like addition (x + y) or multiplication (x y).\n",
    "\n",
    "- FLOPs: floating-point operations (measure of **computation done**)\n",
    "- FLOP/s: floating-point operations per second (also written as FLOP**S**), which is used to measure the **speed of hardware**.\n",
    "\n",
    "#### Several Statistics\n",
    "\n",
    "- GPT-3: `3.14e23` FLOPs\n",
    "\n",
    "- GPT-4: `2e25` FLOPS\n",
    "\n",
    "- A100 has a peak performance of 312 teraFLOP/s. (`teraFLOPS` = 1e12 FLOPS)\n",
    "\n",
    "    - 17806267 hours (total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb525e",
   "metadata": {},
   "source": [
    "#### linear model demo\n",
    "\n",
    "Core: Matrix Multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ccfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    B = 16384  # Number of points\n",
    "    D = 32768  # Dimension\n",
    "    K = 8192   # Number of outputs\n",
    "else:\n",
    "    B = 1024\n",
    "    D = 256\n",
    "    K = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.ones(B, D, device=device)\n",
    "w = torch.randn(D, K, device=device)\n",
    "y = x @ w\n",
    "# We have one multiplication (x[i][j] * w[j][k]) and one addition per (i, j, k) triple.\n",
    "actual_num_flops = 2 * B * D * K  # @inspect actual_num_flops\n",
    "\n",
    "print(actual_num_flops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca4307",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "    \n",
    "- B is the number of data points\n",
    "    \n",
    "- (D K) is the number of parameters\n",
    "    \n",
    "FLOPs for forward pass is 2 (# tokens) (# parameters)\n",
    "    \n",
    "It turns out this generalizes to Transformers (to a first-order approximation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796bd8e",
   "metadata": {},
   "source": [
    "#### Model FLOPS Utilization\n",
    "\n",
    "$ \\text{MFU} = \\frac{\\text{actual FLOPS}}{\\text{promised FLOPS}} $\n",
    "\n",
    "- $ \\text{actual FLOPS} = \\frac{\\text{sum FLOPs}}{\\text{time}} $\n",
    "\n",
    "- $ \\text{promised FLOPS} $ is provided by the hardware company.\n",
    "\n",
    "Usually, MFU of >= 0.5 is quite good (and will be higher if matmuls dominate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2afba6",
   "metadata": {},
   "source": [
    "#### Time Complexity for Several Operations\n",
    "\n",
    "Consider Matrix $A$: $(m,x,k)$ and matrix $B$: $(k,x,n)$.\n",
    "\n",
    "- FLOPs for matrix multiplications: $m \\times n \\times (2k -1)$\n",
    "\n",
    "- Elementwise operation on a $m \\times n$ matrix requires $O(m n)$ FLOPs.\n",
    "    \n",
    "- Addition of two $m \\times n$ matrices requires $m n$ FLOPs.\n",
    "\n",
    "FLOPs depends highly on hardware and data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a3d6cd",
   "metadata": {},
   "source": [
    "### Gradient Basics\n",
    "\n",
    "Computing Gradients also need computation resources!\n",
    "\n",
    "Consider simple linear regression model:\n",
    "\n",
    "```python\n",
    "x = torch.tensor([1., 2, 3])\n",
    "w = torch.tensor([1., 1, 1], requires_grad=True)  # Want gradient\n",
    "pred_y = x @ w\n",
    "loss = 0.5 * (pred_y - 5).pow(2)\n",
    "```\n",
    "\n",
    "Let's get some Math:\n",
    "\n",
    "$$ L(\\vec{w}) = \\frac{1}{2}(\\vec{x} \\cdot \\vec{w} - y_{\\text{true}})^2 $$\n",
    "\n",
    "$$ \\nabla f = \\frac{\\partial L}{\\partial \\vec{w}} = (\\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2}, \\frac{\\partial L}{\\partial w_3}) = (\\vec{x} \\cdot \\vec{w} - y_{\\text{true}}) · (x_1, x_2, x_3) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2, 3])\n",
    "w = torch.tensor([1., 1, 1], requires_grad=True)  # Want gradient\n",
    "\n",
    "# do doct product\n",
    "pred_y = x @ w\n",
    "\n",
    "# MSE Error\n",
    "loss = 0.5 * (pred_y - 5).pow(2)\n",
    "\n",
    "print(x.shape)\n",
    "print(w.shape)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "# run loss backward\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4695a7",
   "metadata": {},
   "source": [
    "### Gradient Flops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83ab96",
   "metadata": {},
   "source": [
    "For the neural network, we can make things more complex.\n",
    "\n",
    "<!-- todo add more complex interpretation for gradient descent. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcead7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    B = 16384  # Number of points\n",
    "    D = 32768  # Dimension\n",
    "    K = 8192   # Number of outputs\n",
    "else:\n",
    "    B = 1024\n",
    "    D = 256\n",
    "    K = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.ones(B, D, device=device)\n",
    "w1 = torch.randn(D, D, device=device, requires_grad=True)\n",
    "w2 = torch.randn(D, K, device=device, requires_grad=True)\n",
    "# Model: x --w1--> h1 --w2--> h2 -> loss\n",
    "h1 = x @ w1\n",
    "h2 = h1 @ w2\n",
    "loss = h2.pow(2).mean()\n",
    "\n",
    "# FLOPs\n",
    "num_forward_flops = (2 * B * D * D) + (2 * B * D * K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175885ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.398e+13\n"
     ]
    }
   ],
   "source": [
    "print(f\"{num_forward_flops:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
