{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16abfe02",
   "metadata": {},
   "source": [
    "# Lecture 2: Pytorch & Resource Accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9257cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40ab85",
   "metadata": {},
   "source": [
    "## Basic Concepts\n",
    "\n",
    "**Efficiency** matters!\n",
    "\n",
    "- Compute: FLOPS\n",
    "\n",
    "- Memory: GB\n",
    "\n",
    "\n",
    "Definition of FLOPS: a metric used to measure the computational power of a computer or processor. It indicates how many **floating-point operations** (calculations involving decimal numbers like addition, subtraction, multiplication, and division) a system can perform per second.\n",
    "\n",
    "$$ \\text{FLOPS (Ideal)} = \\text{Number of Cores} \\times \\text{Clock Frequency per Core} \\times \\text{Floating Point Operations per cycle} $$\n",
    "\n",
    "$$ \\text{FLOPS (Actual)} = \\frac{\\text{Total Number of Floating Point Operations Performed}}{\\text{Execution Times}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14622ab",
   "metadata": {},
   "source": [
    "## Memory Accounting\n",
    "\n",
    "Tensors are the basic building block for storing everything: parameters, gradients, optimizer state, data, activations.\n",
    "\n",
    "[Official Docs](https://docs.pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "Almost everything (parameters, gradients, activations, optimizer states) are stored as floating point numbers.\n",
    "\n",
    "How to compute memory bytes in Tensors?\n",
    "\n",
    "```python\n",
    "def get_memory_usage(x: torch.Tensor):\n",
    "    return x.numel() * x.element_size()\n",
    "\n",
    "# torch.numel: Returns the total number of elements in the input tensor.\n",
    "# torch.element_size: Returns the size in bytes of an individual element.\n",
    "```\n",
    "\n",
    "The result shows how many bytes (1 MB = $2^{20}$ bytes) a tensor is.\n",
    "\n",
    "### Basic Type\n",
    "\n",
    "- `float32`: 1 + 8 + 23, default type\n",
    "- `float16`: 1 + 5 + 10, cuts down the memory\n",
    "- `bfloat16`: 1 + 8 + 7.\n",
    "- `fp8`: 1 + 4 + 3 (FP8E4M3) & 1 + 5 + 2 (FP8E5M2)\n",
    "\n",
    "Google Brain developed bfloat (brain floating point) in 2018 to address this issue. bfloat16 uses the same memory as float16 but has the same dynamic range as float32! The only catch is that the resolution is worse, but this matters less for deep learning.\n",
    "\n",
    "[FP8](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html)\n",
    "\n",
    "Solution: **use mixed precision training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage(x: torch.Tensor):\n",
    "    return x.numel() * x.element_size()\n",
    "\n",
    "\n",
    "# torch.numel: Returns the total number of elements in the input tensor.\n",
    "# torch.element_size: Returns the size in bytes of an individual element.\n",
    "\n",
    "# for float 32\n",
    "x = torch.zeros((4, 8, 20))  # @inspect x\n",
    "print(x.dtype)\n",
    "print(\"Number of elements in this tensor: \", x.numel())\n",
    "print(\"The size of bytes for an individual element in this tensor: \", x.element_size())\n",
    "print(get_memory_usage(x), \"bytes\")\n",
    "print(get_memory_usage(x) / 2**20)\n",
    "\n",
    "# for empty tensor?\n",
    "try:\n",
    "    empty_tensor = torch.empty(4, 8)\n",
    "    print(get_memory_usage(empty_tensor))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# for float 16\n",
    "x = torch.ones((4, 8, 20), dtype=torch.float16)\n",
    "print(x.dtype)\n",
    "print(x.numel())\n",
    "print(x.element_size())\n",
    "# cut the half!\n",
    "print(get_memory_usage(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bytes_information(type):\n",
    "    x = torch.ones((4, 8, 20), dtype=type)\n",
    "    print(f\"=============={type}================\")\n",
    "    print(f\"Dtype: {x.dtype}\")\n",
    "    print(f\"Element size: {x.element_size()}\")\n",
    "    print(f\"Bytes: {get_memory_usage(x)}\")\n",
    "    print(f\"=============={type}================\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "TYPELIST = [torch.float64, torch.float32, torch.float, torch.float16, torch.bfloat16]\n",
    "\n",
    "for type in TYPELIST:\n",
    "    get_bytes_information(type=type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "float32_info = torch.finfo(torch.float32)  # @inspect float32_info\n",
    "float16_info = torch.finfo(torch.float16)  # @inspect float16_info\n",
    "bfloat16_info = torch.finfo(torch.bfloat16)  # @inspect bfloat16_info\n",
    "print(float16_info)\n",
    "print(float32_info)\n",
    "print(bfloat16_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa370ec5",
   "metadata": {},
   "source": [
    "## Compute Accounting\n",
    "\n",
    "### Tensors on GPU\n",
    "\n",
    "By default, tensors are stored in CPU memory. However, in order to take advantage of the massive parallelism of GPUs, we need to move them to GPU memory.\n",
    "\n",
    "![GPU and CPU](https://stanford-cs336.github.io/spring2025-lectures/images/cpu-gpu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic information of GPUs\n",
    "num_gpus = torch.cuda.device_count()  # @inspect num_gpus\n",
    "for i in range(num_gpus):\n",
    "    properties = torch.cuda.get_device_properties(i)  # @inspect properties\n",
    "    print(properties)\n",
    "\n",
    "print(num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x = torch.zeros((4, 8, 10))\n",
    "print(x.device)\n",
    "\n",
    "# moving the cpu to gpu\n",
    "# quite slow if the tensor is large\n",
    "y = x.to(device=torch.device(\"cuda:0\"))\n",
    "\n",
    "\n",
    "def test_time_compute(x: torch.Tensor):\n",
    "    start_time = time.time()\n",
    "    moved = x.to(device=torch.device(\"cuda:0\"))\n",
    "    print(moved.device)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "\n",
    "\n",
    "test_time_compute(torch.zeros(size=(20, 20)))\n",
    "test_time_compute(torch.zeros(size=(50000, 50000)))\n",
    "\n",
    "\n",
    "# creating a tensor directly to gpu\n",
    "memory_allocated = torch.cuda.memory_allocated(\"cuda:1\")\n",
    "time_1 = time.time()\n",
    "z = torch.zeros(size=(50000, 50000), device=\"cuda:1\")\n",
    "time_2 = time.time()\n",
    "print(time_2 - time_1)\n",
    "memory_allocated_new = torch.cuda.memory_allocated(device=\"cuda:1\")\n",
    "memory_used = memory_allocated_new - memory_allocated\n",
    "\n",
    "print(f\"Memory Used: {memory_used}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31075f2",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "\n",
    "#### Tensor Storage\n",
    "\n",
    "PyTorch tensors are pointers into allocated memory, with metadata describing how to get to **any element** of the tensor.\n",
    "\n",
    "For the methods, we use [`.stride()`](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.stride.html)\n",
    "\n",
    "![Stride in torch](https://martinlwx.github.io/img/2D_tensor_strides.png)\n",
    "\n",
    "Stride is the jump necessary to go from one element to the next one in the specified dimension `dim`. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99587984",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.randint(1, 1000, size=(10, 10, 20))\n",
    "print(test_tensor.shape)\n",
    "print(test_tensor.dim())\n",
    "\n",
    "# for the first dimension, it will jump 200 steps for reaching the next element\n",
    "print(test_tensor.stride(0))\n",
    "\n",
    "# for the second dimension, it will jump 20 steps for reaching the next element\n",
    "print(test_tensor.stride(1))\n",
    "\n",
    "# for the last dimension, it will jump 1 step for reaching the next element\n",
    "print(test_tensor.stride(-1))\n",
    "\n",
    "print(test_tensor[2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90ba94",
   "metadata": {},
   "source": [
    "How it works? For example, I want to access the value of `test_tensor[i,j,k]`:\n",
    "I will move: `test_tensor.stride(0) * i + test_tensor.stride(1) * j + test_tensor.stride(2) * k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other operations for tensor: slicing & element_wise\n",
    "# ! all the elementwise operations are operated by single element!\n",
    "x = torch.Tensor([3,3,4])\n",
    "print(x.pow(2))\n",
    "print(x.rsqrt())\n",
    "\n",
    "# `triu` takes the upper triangular part of a matrix.\n",
    "test = torch.randint(1, 1000, size=(2, 2, 2))\n",
    "print(test.triu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9afba62",
   "metadata": {},
   "source": [
    "### Tensor Einops\n",
    "\n",
    "Einops is a library for manipulating tensors where dimensions are named. It is inspired by Einstein summation notation (Einstein, 1916).\n",
    "\n",
    "[Official Docs](https://einops.rocks/1-einops-basics/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# einops demo\n",
    "from einops import rearrange, reduce, repeat\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "# assume there is an image called background.png\n",
    "try:\n",
    "    pil_image = Image.open(\"../../img/background.png\")\n",
    "    original_tensor = transform(pil_image)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "    original_tensor = torch.randn(size=(4, 1144, 1718))\n",
    "\n",
    "original_tensor = original_tensor[:, :1140, :1700]\n",
    "print(original_tensor.shape)\n",
    "# torch.Size([4, 1144, 1718]): (c, h, w)\n",
    "\n",
    "\n",
    "def _to_img(my_tensor, file_path):\n",
    "    if my_tensor.max() > 1.0:\n",
    "        my_tensor = my_tensor / 255.0\n",
    "    to_pil_image = ToPILImage()\n",
    "    pil_image = to_pil_image(my_tensor)\n",
    "    pil_image.save(f\"../../img/{file_path}.png\")\n",
    "\n",
    "\n",
    "rearrange_tensor = rearrange(original_tensor, \"c h w -> c w h\")\n",
    "_to_img(rearrange_tensor, \"rearrange_background\")\n",
    "\n",
    "reduce_tensor = reduce(\n",
    "    original_tensor, \"c (h h2) (w w2) -> c h w\", \"mean\", h2=20, w2=20\n",
    ")\n",
    "# do average pooling (like the CNN) for given tensor\n",
    "print(reduce_tensor.shape)\n",
    "_to_img(reduce_tensor, \"reduce_background\")\n",
    "\n",
    "reduce_tensor = original_tensor[1, :, :].squeeze()\n",
    "print(reduce_tensor.shape)\n",
    "repeat_tensor = repeat(reduce_tensor, \"h w -> c h w\", c=4)\n",
    "_to_img(repeat_tensor, \"repeat_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934580d",
   "metadata": {},
   "source": [
    "#### JaxTyping\n",
    "\n",
    "`jaxtyping` is a Python library that provides type annotations for your array-based code, particularly for the JAX framework. Think of it as a tool that lets you add precise shape and data type information to your function signatures, going far beyond the basic `jax.Array` or `np.ndarray` type hints.\n",
    "\n",
    "For `torch.Tensor`, things get the same.\n",
    "\n",
    "Moreover, jax support JIT and auto-grad functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float\n",
    "x: Float[torch.Tensor, \"batch seq heads hidden\"] = torch.ones(2, 2, 1, 3)  # @inspect x\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61dbff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jaxtyping import Float, Int\n",
    "\n",
    "def matmul(\n",
    "    A: Float[jax.Array, \"batch_size in_features\"],\n",
    "    B: Float[jax.Array, \"in_features out_features\"]\n",
    ") -> Float[jax.Array, \"batch_size out_features\"]:\n",
    "    \"\"\"Performs matrix multiplication.\"\"\"\n",
    "    return A @ B\n",
    "\n",
    "# This will pass type checking\n",
    "A_good = jnp.zeros((128, 784))\n",
    "B_good = jnp.zeros((784, 10))\n",
    "result = matmul(A_good, B_good)\n",
    "print(result.shape) # (128, 10)\n",
    "\n",
    "# A static type checker will flag an error here because \"in_features\"\n",
    "# dimensions don't match (784 vs 600).\n",
    "A_bad = jnp.zeros((128, 784))\n",
    "B_bad = jnp.zeros((600, 10))\n",
    "# result = matmul(A_bad, B_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400745a1",
   "metadata": {},
   "source": [
    "#### `einsum`\n",
    "\n",
    "By using `einops`, we can run this code in a better way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83319311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float\n",
    "from einops import einsum\n",
    "\n",
    "B = 128\n",
    "SEQ1 = 100\n",
    "SEQ2 = 200\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "x: Float[torch.Tensor, \"batch seq1 hidden_dim\"] = torch.randn(size=(B, SEQ1, HIDDEN_DIM))\n",
    "y: Float[torch.Tensor, \"batch seq2 hidden_dim\"] = torch.randn(size=(B, SEQ2, HIDDEN_DIM))\n",
    "\n",
    "z = einsum(x, y, \"batch seq1 hidden_dim, batch seq2 hidden_dim -> batch seq1 seq2\")\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(torch.equal(z, (x @ y.transpose(-2, -1))))\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016eef52",
   "metadata": {},
   "source": [
    "#### `reduce`\n",
    "\n",
    "You can reduce a single tensor via some operation (e.g., sum, mean, max, min).\n",
    "\n",
    "```python\n",
    "x: Float[torch.Tensor, \"batch seq hidden\"] = torch.ones(2, 3, 4)  # @inspect x\n",
    "# Old way:\n",
    "y = x.mean(dim=-1)  # @inspect y\n",
    "# New (einops) way:\n",
    "y = reduce(x, \"... hidden -> ...\", \"sum\")  # @inspect y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "x: Float[torch.Tensor, \"batch seq hidden\"] = torch.randn(2, 3, 4)  # @inspect x\n",
    "\n",
    "# make the last dimension mean to 0\n",
    "x = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "\n",
    "y = reduce(x, \"... hidden -> ...\", \"sum\")  # @inspect y\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f421db7",
   "metadata": {},
   "source": [
    "#### `rearrange`\n",
    "\n",
    "Sometimes, a dimension represents two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x: Float[torch.Tensor, \"batch seq total_hidden\"] = torch.ones(2, 3, 8)  # @inspect x\n",
    "# ...where total_hidden is a flattened representation of heads * hidden1\n",
    "w: Float[torch.Tensor, \"hidden1 hidden2\"] = torch.ones(4, 2)\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "\n",
    "# Break up total_hidden into two dimensions (heads and hidden1):\n",
    "# total_hidden = hidden1 \\times hidden2\n",
    "x = rearrange(x, \"... (heads hidden1) -> ... heads hidden1\", heads=2)  # @inspect x\n",
    "print(f\"x shape: {x.shape}\")\n",
    "\n",
    "# Perform the transformation by w:\n",
    "x = einsum(x, w, \"... hidden1, hidden1 hidden2 -> ... hidden2\")  # @inspect x\n",
    "# Combine heads and hidden2 back together:\n",
    "print(f\"x shape: {x.shape}\")\n",
    "x = rearrange(x, \"... heads hidden2 -> ... (heads hidden2)\")  # @inspect x\n",
    "print(f\"x shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49ffce",
   "metadata": {},
   "source": [
    "### Computation Cost\n",
    "\n",
    "Having gone through all the operations, let us examine their computational cost.\n",
    "\n",
    "**A floating-point operation (FLOP)** is a basic operation like addition (x + y) or multiplication (x y).\n",
    "\n",
    "- FLOPs: floating-point operations (measure of **computation done**)\n",
    "- FLOP/s: floating-point operations per second (also written as FLOP**S**), which is used to measure the **speed of hardware**.\n",
    "\n",
    "#### Several Statistics\n",
    "\n",
    "- GPT-3: `3.14e23` FLOPs\n",
    "\n",
    "- GPT-4: `2e25` FLOPS\n",
    "\n",
    "- A100 has a peak performance of 312 teraFLOP/s. (`teraFLOPS` = 1e12 FLOPS)\n",
    "\n",
    "    - 17806267 hours (total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb525e",
   "metadata": {},
   "source": [
    "#### linear model demo\n",
    "\n",
    "Core: Matrix Multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ccfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    B = 16384  # Number of points\n",
    "    D = 32768  # Dimension\n",
    "    K = 8192   # Number of outputs\n",
    "else:\n",
    "    B = 1024\n",
    "    D = 256\n",
    "    K = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.ones(B, D, device=device)\n",
    "w = torch.randn(D, K, device=device)\n",
    "y = x @ w\n",
    "# We have one multiplication (x[i][j] * w[j][k]) and one addition per (i, j, k) triple.\n",
    "actual_num_flops = 2 * B * D * K  # @inspect actual_num_flops\n",
    "\n",
    "print(actual_num_flops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca4307",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "    \n",
    "- B is the number of data points\n",
    "    \n",
    "- (D K) is the number of parameters\n",
    "    \n",
    "FLOPs for forward pass is 2 (# tokens) (# parameters)\n",
    "    \n",
    "It turns out this generalizes to Transformers (to a first-order approximation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796bd8e",
   "metadata": {},
   "source": [
    "#### Model FLOPS Utilization\n",
    "\n",
    "$ \\text{MFU} = \\frac{\\text{actual FLOPS}}{\\text{promised FLOPS}} $\n",
    "\n",
    "- $ \\text{actual FLOPS} = \\frac{\\text{sum FLOPs}}{\\text{time}} $\n",
    "\n",
    "- $ \\text{promised FLOPS} $ is provided by the hardware company.\n",
    "\n",
    "Usually, MFU of >= 0.5 is quite good (and will be higher if matmuls dominate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2afba6",
   "metadata": {},
   "source": [
    "#### Time Complexity for Several Operations\n",
    "\n",
    "Consider Matrix $A$: $(m,n)$ and matrix $B$: $(n,k)$.\n",
    "\n",
    "- FLOPs for matrix multiplications: $m \\times n \\times (2k)$\n",
    "\n",
    "- Elementwise operation on a $m \\times n$ matrix requires $O(m n)$ FLOPs.\n",
    "    \n",
    "- Addition of two $m \\times n$ matrices requires $m n$ FLOPs.\n",
    "\n",
    "FLOPs depends highly on hardware and data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a3d6cd",
   "metadata": {},
   "source": [
    "### Gradient Basics\n",
    "\n",
    "Computing Gradients also need computation resources!\n",
    "\n",
    "Consider simple linear regression model:\n",
    "\n",
    "```python\n",
    "x = torch.tensor([1., 2, 3])\n",
    "w = torch.tensor([1., 1, 1], requires_grad=True)  # Want gradient\n",
    "pred_y = x @ w\n",
    "loss = 0.5 * (pred_y - 5).pow(2)\n",
    "```\n",
    "\n",
    "Let's get some Math:\n",
    "\n",
    "$$ L(\\vec{w}) = \\frac{1}{2}(\\vec{x} \\cdot \\vec{w} - y_{\\text{true}})^2 $$\n",
    "\n",
    "$$ \\nabla f = \\frac{\\partial L}{\\partial \\vec{w}} = (\\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2}, \\frac{\\partial L}{\\partial w_3}) = (\\vec{x} \\cdot \\vec{w} - y_{\\text{true}}) · (x_1, x_2, x_3) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2, 3])\n",
    "w = torch.tensor([1., 1, 1], requires_grad=True)  # Want gradient\n",
    "\n",
    "# do doct product\n",
    "pred_y = x @ w\n",
    "\n",
    "# MSE Error\n",
    "loss = 0.5 * (pred_y - 5).pow(2)\n",
    "\n",
    "print(x.shape)\n",
    "print(w.shape)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "# run loss backward\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4695a7",
   "metadata": {},
   "source": [
    "### Gradient Flops\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "Just the simple matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83ab96",
   "metadata": {},
   "source": [
    "For the neural network, we can make things more complex.\n",
    "\n",
    "<!-- todo add more complex interpretation for gradient descent. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcead7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384, 32768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    B = 16384  # Number of points\n",
    "    D = 32768  # Dimension\n",
    "    K = 8192   # Number of outputs\n",
    "else:\n",
    "    B = 1024\n",
    "    D = 256\n",
    "    K = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.ones(B, D, device=device)\n",
    "w1 = torch.randn(D, D, device=device, requires_grad=True)\n",
    "w2 = torch.randn(D, K, device=device, requires_grad=True)\n",
    "# Model: x --w1--> h1 --w2--> h2 -> loss\n",
    "h1 = x @ w1\n",
    "print(h1.shape) # (B, D)\n",
    "h2 = h1 @ w2\n",
    "loss = h2.pow(2).mean()\n",
    "\n",
    "# FLOPs\n",
    "# two layers, thus two matrix multiplications\n",
    "num_forward_flops = (2 * B * D * D) + (2 * B * D * K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175885ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.398e+13\n",
      "torch.Size([])\n",
      "torch.Size([16384, 8192])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{num_forward_flops:.3e}\")\n",
    "print(loss.shape)\n",
    "print(h2.shape)\n",
    "\n",
    "h1.retain_grad()\n",
    "h2.retain_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013cfe60",
   "metadata": {},
   "source": [
    "#### Backward Pass\n",
    "\n",
    "Several computation for gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e357aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.ones(B, D, device=device)\n",
    "# w1 = torch.randn(D, D, device=device, requires_grad=True)\n",
    "# w2 = torch.randn(D, K, device=device, requires_grad=True)\n",
    "# # Model: x --w1--> h1 --w2--> h2 -> loss\n",
    "# h1 = x @ w1\n",
    "# print(h1.shape) # (B, D)\n",
    "# h2 = h1 @ w2\n",
    "# loss = h2.pow(2).mean()\n",
    "\n",
    "# x: [B, D]\n",
    "# w1: [D, D]\n",
    "# w2: [D, K]\n",
    "# h1: [B, D]\n",
    "# h2: [B, K]\n",
    "# loss: \\in R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd2992c",
   "metadata": {},
   "source": [
    "**Forward Pass:**\n",
    "$x \\to \\text{w1} \\to \\text{h1} \\to \\text{w2} \\to \\text{h2} \\to \\text{loss}$\n",
    "\n",
    "**Backward Pass:**\n",
    "Starting from the `loss`, the gradient of each parameter is calculated in reverse.\n",
    "\n",
    "The gradient calculation follows the chain rule:\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial w_{2,jk}} = \\sum_{i} \\frac{\\partial \\text{loss}}{\\partial h_{2,ik}} \\cdot \\frac{\\partial h_{2,ik}}{\\partial w_{2,jk}}$$\n",
    "\n",
    "\n",
    "* $\\frac{\\partial \\text{loss}}{\\partial w_{2,jk}}$ is the gradient of the $(j, k)$ element of `w2` that we want to calculate.\n",
    "* $h_{2,ik}$ is the $(i, k)$ element of `h2`.\n",
    "* $\\frac{\\partial \\text{loss}}{\\partial h_{2,ik}}$ is the gradient of the $(i, k)$ element of `h2` (i.e., `h2.grad[i,k]`).\n",
    "* We need to sum over all relevant intermediate variables, which in this case are all rows `i` of `h2`.\n",
    "\n",
    "> It is the element-wise format of:\n",
    "> $$  \\frac{\\partial \\text{loss}}{\\partial \\mathbf{w}_2}  = \\frac{\\partial \\text{loss}}{\\partial \\mathbf{h}_2} \\cdot \\frac{\\partial \\mathbf{h}_2}{\\partial \\mathbf{w}_2} $$\n",
    "\n",
    "`h2` is obtained through the matrix multiplication of `h1` and `w2`.The $(i, k)$ element of `h2`, $h_{2,ik}$, is calculated as:\n",
    "\n",
    "$$h_{2,ik} = \\sum_{t=1}^{p} h_{1,it} \\cdot w_{2,tk}$$\n",
    "\n",
    "Now, we calculate the partial derivative of $h_{2,ik}$ with respect to $w_{2,jk}$.\n",
    "\n",
    "$$\\frac{\\partial h_{2,ik}}{\\partial w_{2,jk}} = \\frac{\\partial}{\\partial w_{2,jk}} \\left( \\sum_{t=1}^{p} h_{1,it} \\cdot w_{2,tk} \\right) = h_{1, ij}$$\n",
    "\n",
    "\n",
    "Substituting our derived $\\frac{\\partial h_{2,ik}}{\\partial w_{2,jk}} = h_{1,ij}$ into the chain rule formula:\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial w_{2,jk}} = \\sum_{i} \\frac{\\partial \\text{loss}}{\\partial h_{2,ik}} \\cdot h_{1,ij}$$\n",
    "\n",
    "Representing this formula using PyTorch terminology, we get the content from the image:\n",
    "$$\\text{w2.grad}[j,k] = \\sum_{i} \\text{h2.grad}[i,k] \\cdot \\text{h1}[i,j]$$\n",
    "\n",
    "For the matrix format:\n",
    "\n",
    "$$ \\mathbf{w}_2.\\text{grad} = \\mathbf{h}_1^{\\top} \\cdot \\mathbf{h}_2.\\text{grad} $$\n",
    "\n",
    "Time cost?\n",
    "\n",
    "```python\n",
    "print(w2.grad.size()) # (D, K)\n",
    "print(h1.size()) # (B, D)\n",
    "print(h2.grad.size()) # (B, K)\n",
    "```\n",
    "\n",
    "Still a matrix multiplication! Time: $ 2B \\times D \\times K $.\n",
    "\n",
    "For the similar algorithms for $\\mathbf{w}_1$, things remain the same.\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial \\text{h1}} = \\frac{\\partial \\text{loss}}{\\partial \\text{h2}} \\cdot \\frac{\\partial \\text{h2}}{\\partial \\text{h1}}$$\n",
    "\n",
    "$$\\text{h1.grad} = \\text{h2.grad} \\cdot \\text{w2}^\\mathsf{T}$$\n",
    "\n",
    "$$\\text{w1.grad} = \\text{x}^\\mathsf{T} \\cdot \\text{h1.grad}$$\n",
    "\n",
    "$$\\text{x.grad} = \\mathbf{h}_1.\\text{grad} \\cdot \\mathbf{w}_1^{\\top}  $$\n",
    "\n",
    "\n",
    "- Compute `h1.grad`: $2BDK$\n",
    "- Compute `w1.grad`: $2BD^2$\n",
    "- Compute `x.grad`: $2BD^2$ (actually unnecessary)\n",
    "\n",
    "Sum for Backward Pass: $4 \\times B \\times (D^2 + DK)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5863b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32768, 8192])\n",
      "torch.Size([32768, 8192])\n",
      "torch.Size([16384, 32768])\n",
      "torch.Size([16384, 32768])\n",
      "torch.Size([16384, 8192])\n",
      "torch.Size([16384, 8192])\n",
      "torch.Size([32768, 32768])\n"
     ]
    }
   ],
   "source": [
    "print(w2.size())\n",
    "print(w2.grad.size()) # (D, K)\n",
    "\n",
    "print(h1.size()) # (B, D)\n",
    "print(h1.grad.size()) # (B, D)\n",
    "\n",
    "print(h2.grad.size()) # (B, K)\n",
    "print(h2.size())\n",
    "\n",
    "print(w1.size()) # (D, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc9ff2",
   "metadata": {},
   "source": [
    "Putting it togther:\n",
    "\n",
    "( # parameters) = the number of all the parameters, which is $w_1$ and $w_2$ in this model ($ D \\times D + D \\times K$).\n",
    "    \n",
    "- Forward pass: 2 (# data points) (# parameters) FLOPs = $2B(D^2 + DK)$\n",
    "    \n",
    "- Backward pass: 4 (# data points) (# parameters) FLOPs = $4B(D^2 + DK)$\n",
    "    \n",
    "- Total: 6 (# data points) (# parameters) FLOPs\n",
    "\n",
    "Thus:\n",
    "\n",
    "> Question: How long would it take to train a 70B parameter model on 15T tokens on 1024 H100s?\n",
    "> total_flops = 6 * 70e9 * 15e12  # @inspect total_flops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88f7ff",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Module Parameters\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "When the input dimension `input_dim` of a neural network layer is large, its output values also tend to become very large.\n",
    "\n",
    "For example, `output = x @ w` describes a multiplication of a vector `x` by a weight matrix `w`. If the elements of both `x` and `w` are **sampled randomly from a standard normal distribution**, then according to probability theory, the variance of each element in the output `output` is proportional to `input_dim`.\n",
    "\n",
    "For example, the $j$-th element of `output` is:\n",
    "$$\\text{output}_j = \\sum_{i=1}^{\\text{input\\_dim}} x_i \\cdot w_{ij}$$\n",
    "\n",
    "If $x_i$ and $w_{ij}$ have a mean of 0 and a variance of 1, then the variance of $\\text{output}_j$ is:\n",
    "\n",
    "$$\\text{Var}(\\text{output}_j) = \\sum_{i=1}^{\\text{input\\_dim}} \\text{Var}(x_i \\cdot w_{ij}) $$\n",
    "$$= \\sum_{i=1}^{\\text{input\\_dim}} \\text{Var}(x_i) \\cdot \\text{Var}(w_{ij}) = \\text{input\\_dim} \\cdot 1 \\cdot 1 = \\text{input\\_dim}$$\n",
    "\n",
    "This means the standard deviation of `output` is $\\sqrt{\\text{input\\_dim}}$. This could cause damages when the input dim scales larger, for instance: gradient vanishing, etc.\n",
    "\n",
    "> [Xavier Initialization](https://www.youtube.com/watch?v=ScWTYHQra5E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6a58c",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "In language modeling, data is a sequence of integers (output by the tokenizer).\n",
    "It is convenient to serialize them as numpy arrays (done by the tokenizer).\n",
    "\n",
    "```python\n",
    "orig_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.int32)\n",
    "orig_data.tofile(\"data.npy\")\n",
    "# You can load them back as numpy arrays.\n",
    "# Don't want to load the entire data into memory at once (LLaMA data is 2.8TB).\n",
    "# Use memmap to lazily load only the accessed parts into memory.\n",
    "data = np.memmap(\"data.npy\", dtype=np.int32)\n",
    "assert np.array_equal(data, orig_data)\n",
    "# A data loader generates a batch of sequences for training.\n",
    "B = 2  # Batch size\n",
    "L = 4  # Length of sequence\n",
    "x = get_batch(data, batch_size=B, sequence_length=L, device=get_device())\n",
    "assert x.size() == torch.Size([B, L])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8c9883",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Let's define the AdaGrad optimizer\n",
    "    \n",
    "- momentum = SGD + exponential averaging of grad\n",
    "    \n",
    "- AdaGrad = SGD + averaging by grad^2\n",
    "    \n",
    "- RMSProp = AdaGrad + exponentially averaging of grad^2\n",
    "    \n",
    "- Adam = RMSProp + momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78d3c9",
   "metadata": {},
   "source": [
    "\n",
    "#### SGD (Stochastic Gradient Descent)\n",
    "\n",
    "This is the most basic optimization method. It updates model parameters by calculating the gradient of the loss function and moving in the opposite direction of the gradient.\n",
    "\n",
    "$$w_{t+1} = w_t - \\eta \\cdot g_t$$\n",
    "Where:\n",
    "* $w_t$ is the parameter at the current time step.\n",
    "* $\\eta$ is the learning rate, a hyperparameter that controls the step size.\n",
    "* $g_t$ is the **gradient at the current time step**.\n",
    "\n",
    "**Main Problem**: The learning rate is fixed. This can cause the optimizer to move too fast in steep areas and too slow in flat areas. It also tends to oscillate in certain directions.\n",
    "\n",
    "\n",
    "#### AdaGrad (Adaptive Gradient Algorithm)\n",
    "\n",
    "AdaGrad is an improvement over SGD that introduces the concept of an **adaptive learning rate**. It uses a different learning rate for each parameter, and these rates decay over time during training.\n",
    "\n",
    "**Core Idea**: It scales the learning rate for each parameter by accumulating the sum of the squares of all past gradients.\n",
    "$$G_t = \\sum_{\\tau=1}^{t} g_{\\tau}^2$$\n",
    "$$w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot g_t$$\n",
    "Where:\n",
    "* $G_t$ is the sum of the squared gradients from the start of training up to the current time step.\n",
    "* $\\epsilon$ is a small constant to prevent division by zero.\n",
    "\n",
    "**Advantage**: It's well-suited for sparse data, as parameters that are updated infrequently get a larger learning rate.\n",
    "\n",
    "**Main Problem**: Since $G_t$ is a monotonically increasing sum, the learning rate for all parameters will eventually become **extremely small**, halting further learning in the later stages of training.\n",
    "\n",
    "\n",
    "#### RMSProp (Root Mean Square Propagation)\n",
    "\n",
    "RMSProp addresses the issue of AdaGrad's rapidly decaying learning rate. Instead of accumulating the sum of all past squared gradients, it uses an **exponentially weighted moving average** of the squared gradients. This allows it to \"forget\" distant past gradients, preventing the learning rate from decaying too quickly.\n",
    "\n",
    "**Core Idea**: It replaces AdaGrad's cumulative sum with an exponential moving average.\n",
    "$$E[g^2]_t = \\gamma \\cdot E[g^2]_{t-1} + (1-\\gamma) \\cdot g_t^2$$\n",
    "$$w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot g_t$$\n",
    "Where:\n",
    "* $E[g^2]_t$ is the exponentially weighted moving average of the squared gradients.\n",
    "* $\\gamma$ is a decay rate, typically set to 0.9 or 0.99.\n",
    "\n",
    "**Advantage**: It solves the premature learning rate decay problem of AdaGrad, allowing the model to continue learning throughout training.\n",
    "\n",
    "\n",
    "#### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Adam combines the best of RMSProp and **Momentum**. It maintains an exponentially weighted moving average of both the gradients (the momentum term) and the squared gradients (the RMSProp term).\n",
    "\n",
    "**Core Idea**:\n",
    "* **First moment**: An exponentially weighted moving average of the gradients (the momentum term).\n",
    "    $$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n",
    "* **Second moment**: An exponentially weighted moving average of the squared gradients (the RMSProp term).\n",
    "    $$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$$\n",
    "    $$ v_{t}=\\left(1-\\beta_{2}\\right) \\sum_{i=1}^{t} \\beta_{2}^{t-i} g_{i}^{2} $$\n",
    "\n",
    "> v represents variance.\n",
    "> $ \\text{Var}(g) = \\mathbb{E}[g^2] - (\\mathbb{E}[g])^2 $, and $v_t$ represents the moving average squared gradients.\n",
    "> Thus we have: $\\text{Var}(g) \\approx v_t - (m_t)^2 $\n",
    "> $v_t$ and $m_t$ are all moving average of $g^2$ and $g$, which in essence is a **weighted average**.\n",
    "\n",
    "\n",
    "* Adam also includes a bias correction, as $m_t$ and $v_t$ are biased towards zero in the initial training steps.\n",
    "* The final update rule  after bias correction is:\n",
    "    $$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$   $$w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t$$\n",
    "\n",
    "**Advantage**: It combines the benefits of momentum and adaptive learning rates, leading to faster convergence and often superior performance. Adam is a highly popular and widely used optimizer for a wide range of deep learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917759b",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0a8dac",
   "metadata": {},
   "source": [
    "```python\n",
    "# Parameters\n",
    "num_parameters = (D * D * num_layers) + D  # @inspect num_parameters\n",
    "assert num_parameters == get_num_parameters(model)\n",
    "# Activations\n",
    "num_activations = B * D * num_layers  # @inspect num_activations\n",
    "# Gradients\n",
    "num_gradients = num_parameters  # @inspect num_gradients\n",
    "# Optimizer states\n",
    "num_optimizer_states = num_parameters  # @inspect num_optimizer_states\n",
    "# For Adam, it is 2 * num_parameters\n",
    "\n",
    "# Putting it all together, assuming float32\n",
    "total_memory = 4 * (num_parameters + num_activations + num_gradients + num_optimizer_states)  # @inspect total_memory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b63fee",
   "metadata": {},
   "source": [
    "### Checkpoint\n",
    "\n",
    "Training language models take a long time and  will certainly crash. You don't want to lose all your progress.\n",
    "During training, it is useful to periodically save your model and optimizer state to disk.\n",
    "```python\n",
    "model = Cruncher(dim=64, num_layers=3).to(get_device())\n",
    "optimizer = AdaGrad(model.parameters(), lr=0.01)\n",
    "# Save the checkpoint:\n",
    "checkpoint = {\n",
    "    \"model\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "}\n",
    "torch.save(checkpoint, \"model_checkpoint.pt\")\n",
    "# Load the checkpoint:\n",
    "loaded_checkpoint = torch.load(\"model_checkpoint.pt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676ab03",
   "metadata": {},
   "source": [
    "## Mixed Precision Training\n",
    "\n",
    "Choice of data type (float32, bfloat16, fp8) have tradeoffs.\n",
    "    \n",
    "- Higher precision: more accurate/stable, more memory, more compute\n",
    "    \n",
    "- Lower precision: less accurate/stable, less memory, less compute\n",
    "\n",
    "\n",
    "Solution: use float32 by default, but use {bfloat16, fp8} when possible.\n",
    "A concrete plan:\n",
    "    \n",
    "- Use {bfloat16, fp8} for the forward pass (activations).\n",
    "    \n",
    "- Use float32 for the rest (parameters, gradients).\n",
    "    \n",
    "Pytorch has an automatic mixed precision (AMP) library.\n",
    "- https://pytorch.org/docs/stable/amp.html\n",
    "- https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5fec26",
   "metadata": {},
   "source": [
    "## Transformers for Resource Accounting\n",
    "\n",
    "- [Transformer Memory](https://erees.dev/transformer-memory/)\n",
    "\n",
    "- [Transformer FLOPs](https://www.adamcasson.com/posts/transformer-flops)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
