{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d98563",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eba875f",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "\n",
    "Install `nltk` and download its data.\n",
    "\n",
    "```bash\n",
    "pip install nltk\n",
    "python -m nltk.downloader punkt\n",
    "```\n",
    "\n",
    "Then it will download the data in the home directory:\n",
    "```text\n",
    "[nltk_data] Downloading package punkt to\n",
    "[nltk_data]     /GPFS/rhome/xiyuanyang/nltk_data...\n",
    "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861066e",
   "metadata": {},
   "source": [
    "## Quick Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# step1: download some data\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "text = \"This is an example sentence, with punctuation!\"\n",
    "words = word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "# for some chinese demo\n",
    "text_chn = \"我是上海交通大学的一名学生,你觉的我怎么样，I am very happy to see you!\"\n",
    "# nltk is quite foolish...\n",
    "words_chn = word_tokenize(text_chn)\n",
    "print(words_chn)\n",
    "\n",
    "# for chinses, you can use jieba\n",
    "import jieba\n",
    "\n",
    "\n",
    "text = \"我爱北京天安门，天安门上太阳升。\"\n",
    "# 精确模式\n",
    "seg_list_precise = jieba.cut(text, cut_all=False)\n",
    "print(\"精确模式:\", \" \".join(seg_list_precise))\n",
    "\n",
    "# 全模式\n",
    "seg_list_all = jieba.cut(text, cut_all=True)\n",
    "print(\"全模式:\", \" \".join(seg_list_all))\n",
    "\n",
    "# 搜索引擎模式\n",
    "seg_list_search = jieba.cut_for_search(text)\n",
    "print(\"搜索引擎模式:\", \" \".join(seg_list_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8357dcb",
   "metadata": {},
   "source": [
    "## Encoding and Decoding\n",
    "\n",
    "We will use `spacy` for more advanced NLP usage.\n",
    "\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "```text\n",
    "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "Collecting en-core-web-sm==3.8.0\n",
    "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
    "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 728.1 kB/s eta 0:00:00\n",
    "Installing collected packages: en-core-web-sm\n",
    "Successfully installed en-core-web-sm-3.8.0\n",
    "✔ Download and installation successful\n",
    "You can now load the package via spacy.load('en_core_web_sm')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27afc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"This is an example sentence, with punctuation!\"\n",
    "doc = nlp(text)\n",
    "\n",
    "words = [token.text for token in doc]\n",
    "print(words)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} {token.pos_:<10} {token.is_punct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11958afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for encoding and decoding\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Hello world, I am Xiyuan Yang, a freshman in Shanghai JiaoTong University.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get the tokenized words (tokens)\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"SpaCy Tokens:\", tokens)\n",
    "\n",
    "# --- Encode (Get Token Hashes or IDs) ---\n",
    "# spaCy internally uses hash values to represent strings, and these hashes map to integer IDs.\n",
    "# token.norm is the normalized hash value (lowercase, etc.)\n",
    "# token.orth is the hash value of the original string (case-sensitive).\n",
    "\n",
    "token_ids = [nlp.vocab.strings.as_int(token.text) for token in doc]\n",
    "print(\"Token IDs (encoded):\", token_ids)\n",
    "\n",
    "\n",
    "# --- Decode (Convert IDs back to Tokens) ---\n",
    "decoded_tokens = [nlp.vocab.strings.as_string(id_val) for id_val in token_ids]\n",
    "print(\"Decoded Tokens:\", decoded_tokens)\n",
    "\n",
    "# --- More advanced Vocab usage ---\n",
    "# The `nlp.vocab.strings` object is a StringStore, which manages all string-to-ID mappings.\n",
    "print(\"\\nSpaCy Vocab Example:\")\n",
    "print(\"Current vocab size:\", len(nlp.vocab))\n",
    "\n",
    "# Add a new word to the vocabulary and Get the ID of the new word\n",
    "new_word = \"neural_network\"\n",
    "nlp.vocab.strings.add(new_word)\n",
    "new_word_id = nlp.vocab.strings.as_int(new_word)\n",
    "print(f\"Added '{new_word}', its ID is: {new_word_id}\")\n",
    "\n",
    "# Decode the ID back to the string\n",
    "decoded_new_word = nlp.vocab.strings.as_string(new_word_id)\n",
    "print(f\"Decoding ID {new_word_id}: {decoded_new_word}\")\n",
    "\n",
    "# If I change the hash value...\n",
    "try:\n",
    "    modified_id = new_word_id + 1\n",
    "    modified_word = nlp.vocab.strings.as_string(modified_id)\n",
    "    print(f\"After Modification: {modified_word}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee303614",
   "metadata": {},
   "source": [
    "## Advanced Tokenizer\n",
    "\n",
    "We will use `tiktoken` for advanced tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903535fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Choose an encoding that matches the models you might be using (e.g., for GPT-4, GPT-3.5-turbo)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "text_en = \"Hello, world! This is a test sentence for tiktoken.\"\n",
    "\n",
    "# --- Encode (Text to Token IDs) ---\n",
    "# encode() converts the string into a list of integer token IDs\n",
    "token_ids_en = encoding.encode(text_en)\n",
    "print(f\"Original English Text: '{text_en}'\")\n",
    "print(f\"Encoded Token IDs (English): {token_ids_en}\")\n",
    "print(f\"Number of tokens (English): {len(token_ids_en)}\")\n",
    "\n",
    "# To see the actual tokens that correspond to the IDs (optional, for understanding)\n",
    "# This requires decoding each ID individually or using a helper.\n",
    "# Note: decoding individual IDs might not always yield readable strings if tokens are sub-word units.\n",
    "decoded_parts_en = [encoding.decode([token_id]) for token_id in token_ids_en]\n",
    "print(f\"Decoded Parts (English, for understanding): {decoded_parts_en}\")\n",
    "\n",
    "# --- Decode (Token IDs to Text) ---\n",
    "# decode() converts a list of integer token IDs back into a string\n",
    "decoded_text_en = encoding.decode(token_ids_en)\n",
    "print(f\"Decoded English Text: '{decoded_text_en}'\")\n",
    "\n",
    "decoded_modified = [encoding.decode([token_id + 1]) for token_id in token_ids_en]\n",
    "print(decoded_modified)\n",
    "\n",
    "# Check if decoded text matches original (should be True)\n",
    "print(f\"Decoded matches original? {decoded_text_en == text_en}\")\n",
    "print(f\"After Modifications? {decoded_modified == text_en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "204439f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', ',', ' My', ' name', ' is', ' X', 'iy', 'uan', ' Yang']\n",
      "3.5\n",
      "['wow', ',', ' it', ' is', ' so', ' fantastic', '!']\n",
      "3.4285714285714284\n",
      "['你', '好', '，', '这', '里', '是', '中', '文', '，', '自', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '，', '我', '言', '�', '�', '日', '�', '�', '�', '�', '�', '�']\n",
      "2.057142857142857\n",
      "['international', ' computational']\n",
      "13.5\n",
      "['#', ' Learn', ' L', 'LM', 's', ' from', ' scratch', '\\n\\n', '>', ' [', '!', 'WARNING', ']\\n', '>', ' Still', ' in', ' the', ' process', ' of', ' **', 'ref', 'actoring', '**', '!\\n\\n', '##', ' Table', ' of', ' Contents', '\\n\\n', 'See', ' [', 'This', ' Blog', '](', 'https', '://', 'xi', 'yu', 'any', 'ang', '-code', '.github', '.io', '/posts', '/', 'LL', 'M', '-L', 'earning', '-', 'Initial', '/)', ' for', ' more', ' tutorials', '.\\n\\n\\n', '##', ' L', 'LM', ' Learning', ' Materials', '\\n\\n', '-', ' Courses', ' and', ' Videos', '\\n\\n', '   ', ' -', ' [', 'Post', ' Training', ' by', ' Deep', 'Learning', ' Ai', '](', 'https', '://', 'www', '.de', 'e', 'ple', 'arning', '.ai', '/', 'short', '-c', 'ourses', '/post', '-training', '-of', '-', 'll', 'ms', '/', ')\\n\\n', '   ', ' -', ' [', 'CS', '25', ':', ' Transformers', ' United', ' V', '5', '](', 'https', '://', 'web', '.st', 'anford', '.edu', '/class', '/cs', '25', '/', '):', ' For', ' advanced', ' architectures', ' for', ' transformers', '.\\n\\n', '   ', ' -', ' [', 'Transform', 'ers', ' for', ' ', '3', 'b', '1', 'b', '](', 'https', '://', 'www', '.youtube', '.com', '/', 'playlist', '?', 'list', '=', 'PL', 'Z', 'HQ', 'Ob', 'OW', 'T', 'Q', 'D', 'NU', '6', 'R', '1', '_', '670', '00', 'D', 'x', '_Z', 'C', 'JB', '-', '3', 'pi', '):', ' A', ' good', ' visualize', ' demo', '!\\n\\n', '   ', ' -', ' [', '**', 'Advanced', ' Natural', ' Language', ' Processing', '**', '](', 'https', '://', 'phon', 'tron', '.com', '/class', '/an', 'lp', '-f', 'all', '202', '4', '/', '):', ' A', ' very', ' good', ' course', ' for', ' detailed', ' lecture', ' notes', ' and', ' homework', '.', ' (', 'I', ' will', ' try', ' to', ' finish', ' that', ' project', ' in', ' the', ' next', ' semester', '.)\\n\\n', '   ', ' -', ' [', 'LL', 'Ms', ' and', ' Transformers', '](', 'https', '://', 'www', '.', 'amb', 'uj', 't', 'ew', 'ari', '.com', '/', 'LL', 'M', '-f', 'all', '202', '4', '/#', 'log', 'istics', '--', 'schedule', '):', ' with', ' several', ' discussion', ' topics', ' (', 'lecture', ' notes', ',', ' blogs', ' and', ' papers', ' are', ' included', ')\\n\\n', '   ', ' -', ' [', 'D', 'ive', ' into', ' L', 'LM', 's', '](', 'https', '://', 'sj', 't', 'ull', 'm', '.git', 'book', '.io', '/d', 'ive', '-', 'into', '-', 'll', 'ms', '):', ' The', ' chinese', ' version', ' of', ' learning', ' large', ' language', ' models', '\\n\\n', '   ', ' -', ' [', '**', 'Learning', ' Large', ' Language', ' Models', ' from', ' scratch', '**', '](', 'https', '://', 'stan', 'ford', '-c', 's', '336', '.github', '.io', '/s', 'pring', '202', '5', '/', '):', ' The', ' course', ' for', ' learning', ' L', 'LM', 's', ' in', ' stan', 'ford', '.\\n\\n', '   ', ' -', ' [', 'Large', ' Language', ' Models', ' for', ' Data', 'Wh', 'ale', '](', 'https', '://', 'www', '.data', 'wh', 'ale', '.cn', '/', 'learn', '/', 'summary', '/', '107', '):', ' Courses', ' of', ' Chinese', ' version', '.\\n\\n', '-', ' Several', ' books', ' and', ' codes', ':', ' \\n\\n', '   ', ' -', ' Hands', ' on', ' Large', ' Language', ' Models', ' (', 'English', ' Version', ')\\n\\n', '       ', ' -', ' Original', ' English', ' Version', ':', ' [', 'Hands', ' on', ' Large', ' Language', ' Models', '](', 'https', '://', 'github', '.com', '/', 'Hands', 'On', 'LL', 'M', '/', 'Hands', '-On', '-L', 'arge', '-Language', '-', 'Models', ')\\n\\n', '       ', ' -', ' https', '://', 'www', '.ll', 'm', '-book', '.com', '/\\n\\n', '   ', ' -', ' Hands', ' on', ' Large', ' Language', ' Models', ' (', 'CH', 'N', ' Version', ')\\n\\n', '       ', ' -', ' [', 'Source', ' Code', '](', 'https', '://', 'github', '.com', '/b', 'br', 'uce', 'y', 'uan', '/', 'Hands', '-On', '-L', 'arge', '-Language', '-', 'Models', '-CN', ')\\n\\n', '   ', ' -', ' Build', ' a', ' Large', ' Language', ' Model', ' (', 'From', ' Scratch', ')\\n\\n', '       ', ' -', ' [', 'Github', ' Repo', '](', 'https', '://', 'github', '.com', '/r', 'as', 'bt', '/', 'LL', 'Ms', '-from', '-s', 'cratch', ')\\n\\n', '       ', ' -', ' [', 'Additional', ' Technical', ' Blog', '](', 'https', '://', 'mag', 'azine', '.se', 'bastian', 'ras', 'ch', 'ka', '.com', '/p', '/', 'll', 'm', '-re', 'search', '-p', 'apers', '-the', '-', '202', '4', '-list', ')\\n\\n', '-', ' Projects', ':', ' \\n\\n', '   ', ' -', ' L', 'LM', ' Hero', ' to', ' Zero', ':\\n\\n', '       ', ' -', ' Build', ' a', ' simple', ' G', 'PT', ' from', ' scratch', '.\\n\\n', '       ', ' -', ' [', 'LL', 'M', ' from', ' hero', ' to', ' zero', ',', ' kar', 'path', 'y', ' version', '](', 'https', '://', 'kar', 'path', 'y', '.ai', '/', 'zero', '-to', '-hero', '.html', ')\\n\\n', '           ', ' -', ' [', 'Source', ' Code', '](', 'https', '://', 'github', '.com', '/k', 'ar', 'path', 'y', '/ng', '-video', '-', 'lecture', ')\\n\\n', '           ', ' -', ' [', 'L', 'ect', 'ures', ' Videos', '](', 'https', '://', 'www', '.youtube', '.com', '/watch', '?v', '=k', 'Cc', '8', 'F', 'm', 'Eb', '1', 'n', 'Y', ')\\n\\n', '       ', ' -', ' [', 'LL', 'M', ' from', ' hero', ' to', ' zero', ',', ' CH', 'N', ' version', '](', 'https', '://', 'yu', 'anch', 'a', 'of', 'a', '.com', '/', 'll', 'ms', '-zero', '-to', '-hero', '/', ')\\n\\n', '           ', ' -', ' [', 'Source', ' Code', '](', 'https', '://', 'github', '.com', '/b', 'br', 'uce', 'y', 'uan', '/', 'LL', 'Ms', '-Z', 'ero', '-to', '-H', 'ero', ')\\n\\n', '-', ' Tool', ' Usage', '\\n\\n', '   ', ' -', ' H', 'ugging', 'Face', ' for', ' downloading', ' models', ' and', ' datasets', '\\n\\n', '   ', ' -', ' [', 'v', 'll', 'm', '](', 'https', '://', 'github', '.com', '/v', 'll', 'm', '-project', '/v', 'll', 'm', ')\\n\\n', '##', ' L', 'LM', ' Learning', ' Contents', '\\n\\n', '-', ' Basic', ' architecture', ' for', ' Large', ' Language', ' Models', '\\n\\n', '   ', ' -', ' Attention', ' Mechan', 'ism', ' (', 'Attention', ' is', ' all', ' you', ' need', '!)\\n\\n', '   ', ' -', ' R', 'NN', ',', ' LSTM', ',', ' GR', 'U', '\\n\\n', '   ', ' -', ' Seq', '2', 'Seq', ' Model', '\\n\\n', '   ', ' -', ' Transformer', ' Architecture', '\\n\\n', '-', ' Pre', ' Training', ' for', ' L', 'LM', '\\n\\n', '   ', ' -', ' Loading', ' D', 'atasets', '\\n\\n', '   ', ' -', ' Self', '-sup', 'ervised', ' Learning', '\\n\\n', '   ', ' -', ' More', ' advanced', ' architecture', ' for', ' L', 'LM', ' pre', '-training', ',', ' see', ' advanced', ' structure', ' part', '.\\n\\n', '-', ' Post', ' Training', ' for', ' L', 'LM', '\\n\\n', '   ', ' -', ' Quant', 'ization', ' for', ' Model', ' Optimization', '\\n\\n', '   ', ' -', ' Knowledge', ' Dist', 'illation', '\\n\\n', '   ', ' -', ' Fine', '-t', 'uning', ' Techniques', '\\n', '       ', ' -', ' S', 'FT', '\\n', '       ', ' -', ' R', 'FT', '\\n', '       ', ' -', ' RL', 'HF', ' (', 'Re', 'in', 'forcement', ' Learning', ' from', ' Human', ' Feedback', ')\\n\\n', '   ', ' -', ' L', 'LM', ' Evaluation', '\\n\\n', '-', ' Advanced', ' Structure', ' for', ' L', 'LM', '\\n\\n', '   ', ' -', ' [', 'Advanced', ' Transformer', '](', 'https', '://', 'spaces', '.ac', '.cn', '/search', '/', 'Transformer', '%E', '5', '%', '8', 'D', '%', '87', '%E', '7', '%', 'BA', '%A', '7', '%E', '4', '%B', '9', '%', '8', 'B', '%E', '8', '%B', '7', '%', 'AF', '/', ')\\n\\n', '   ', ' -', ' Sparse', ' Attention', ' &', ' Lightning', ' Attention', '\\n\\n', '   ', ' -', ' KV', ' cache', '\\n\\n', '   ', ' -', ' M', 'ixture', ' of', ' Experts', ' (', 'Mo', 'E', ')\\n', '       ', ' -', ' [', 'Mo', 'E', ' Introduction', '](', 'https', '://', 'mp', '.weixin', '.qq', '.com', '/s', '/k', 'UF', '4', 'cy', '1', 'QA', '_x', 'Q', 'Sy', 'T', '5', 'H', 'tc', 'K', 'IA', ')\\n\\n', '       ', ' -', ' [', 'Mo', 'E', ' Advanced', '](', 'https', '://', 'yu', 'anch', 'a', 'of', 'a', '.com', '/', 'll', 'ms', '-zero', '-to', '-hero', '/the', '-way', '-of', '-m', 'oe', '-model', '-ev', 'olution', '.html', '#', '_', '2', '-%', 'E', '7', '%', '89', '%', '88', '%E', '6', '%', '9', 'C', '%', 'AC', '2', '-s', 'parse', 'm', 'oe', '-%', 'E', '5', '%A', '4', '%A', '7', '%E', '6', '%A', '8', '%A', '1', '%E', '5', '%', '9', 'E', '%', '8', 'B', '%E', '8', '%', 'AE', '%', 'AD', '%E', '7', '%', 'BB', '%', '83', '%E', '4', '%', 'BD', '%', 'BF', '%E', '7', '%', '94', '%A', '8', ')\\n\\n', '   ', ' -', ' Lo', 'RA', ':', ' Low', '-R', 'ank', ' Adapt', 'ation', ' of', ' Large', ' Language', ' Models', '\\n\\n', '   ', ' -', ' P', 'PO', ',', ' GR', 'PO', ',', ' D', 'PO', ',', ' etc', '.', ' (', 'Deep', ' Rein', 'forcement', ' Learning', ')\\n\\n', '-', ' Test', ' time', ' compute', ' for', ' L', 'LM', ' (', 'after', ' training', ')\\n\\n', '   ', ' -', ' L', 'LM', ' Reason', 'ing', ' (', 'Co', 'T', ',', ' To', 'T', ',', ' etc', '.)\\n\\n', '       ', ' -', ' Recommend', ' Blog', ':', ' [', 'Why', ' we', ' think', ' by', ' Lil', 'ian', ' W', 'eng', '](', 'https', '://', 'l', 'ilian', 'w', 'eng', '.github', '.io', '/posts', '/', '202', '5', '-', '05', '-', '01', '-thinking', '/', ')\\n\\n', '-', ' L', 'LM', ' Down', 'Stream', ' Applications', '\\n\\n', '   ', ' -', ' This', ' section', ' will', ' be', ' recorded', ' in', ' the', ' future', '.\\n\\n', '   ', ' -', ' R', 'AG', '\\n\\n', '   ', ' -', ' Lang', 'Chain', ' Community', '\\n\\n']\n",
      "3.562925170068027\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "test_strings = [\n",
    "    \"Hello world, My name is Xiyuan Yang\",\n",
    "    \"wow, it is so fantastic!\",\n",
    "    \"你好，这里是中文，自古逢秋悲寂寥，我言秋日胜春朝\",\n",
    "    \"international computational\"\n",
    "]\n",
    "with open(\"../../README.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    long_string = file.read()\n",
    "    file.close()\n",
    "test_strings.append(long_string)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for test_string in test_strings:\n",
    "    tokens = encoding.encode(test_string)\n",
    "    num_bytes = len(bytes(test_string, encoding=\"utf-8\"))\n",
    "    # print(num_bytes)\n",
    "    num_tokens = len(tokens)\n",
    "    decoded = [encoding.decode([token]) for token in tokens]\n",
    "    print(decoded)\n",
    "    print(num_bytes / num_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
