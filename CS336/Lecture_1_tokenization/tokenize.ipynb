{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d98563",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eba875f",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "\n",
    "Install `nltk` and download its data.\n",
    "\n",
    "```bash\n",
    "pip install nltk\n",
    "python -m nltk.downloader punkt\n",
    "```\n",
    "\n",
    "Then it will download the data in the home directory:\n",
    "```text\n",
    "[nltk_data] Downloading package punkt to\n",
    "[nltk_data]     /GPFS/rhome/xiyuanyang/nltk_data...\n",
    "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861066e",
   "metadata": {},
   "source": [
    "## Quick Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# step1: download some data\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "text = \"This is an example sentence, with punctuation!\"\n",
    "words = word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "# for some chinese demo\n",
    "text_chn = \"我是上海交通大学的一名学生,你觉的我怎么样，I am very happy to see you!\"\n",
    "# nltk is quite foolish...\n",
    "words_chn = word_tokenize(text_chn)\n",
    "print(words_chn)\n",
    "\n",
    "# for chinses, you can use jieba\n",
    "import jieba\n",
    "\n",
    "\n",
    "text = \"我爱北京天安门，天安门上太阳升。\"\n",
    "# 精确模式\n",
    "seg_list_precise = jieba.cut(text, cut_all=False)\n",
    "print(\"精确模式:\", \" \".join(seg_list_precise))\n",
    "\n",
    "# 全模式\n",
    "seg_list_all = jieba.cut(text, cut_all=True)\n",
    "print(\"全模式:\", \" \".join(seg_list_all))\n",
    "\n",
    "# 搜索引擎模式\n",
    "seg_list_search = jieba.cut_for_search(text)\n",
    "print(\"搜索引擎模式:\", \" \".join(seg_list_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8357dcb",
   "metadata": {},
   "source": [
    "## Encoding and Decoding\n",
    "\n",
    "We will use `spacy` for more advanced NLP usage.\n",
    "\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "```text\n",
    "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "Collecting en-core-web-sm==3.8.0\n",
    "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
    "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 728.1 kB/s eta 0:00:00\n",
    "Installing collected packages: en-core-web-sm\n",
    "Successfully installed en-core-web-sm-3.8.0\n",
    "✔ Download and installation successful\n",
    "You can now load the package via spacy.load('en_core_web_sm')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27afc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"This is an example sentence, with punctuation!\"\n",
    "doc = nlp(text)\n",
    "\n",
    "words = [token.text for token in doc]\n",
    "print(words)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} {token.pos_:<10} {token.is_punct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11958afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for encoding and decoding\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Hello world, I am Xiyuan Yang, a freshman in Shanghai JiaoTong University.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get the tokenized words (tokens)\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"SpaCy Tokens:\", tokens)\n",
    "\n",
    "# --- Encode (Get Token Hashes or IDs) ---\n",
    "# spaCy internally uses hash values to represent strings, and these hashes map to integer IDs.\n",
    "# token.norm is the normalized hash value (lowercase, etc.)\n",
    "# token.orth is the hash value of the original string (case-sensitive).\n",
    "\n",
    "token_ids = [nlp.vocab.strings.as_int(token.text) for token in doc]\n",
    "print(\"Token IDs (encoded):\", token_ids)\n",
    "\n",
    "\n",
    "# --- Decode (Convert IDs back to Tokens) ---\n",
    "decoded_tokens = [nlp.vocab.strings.as_string(id_val) for id_val in token_ids]\n",
    "print(\"Decoded Tokens:\", decoded_tokens)\n",
    "\n",
    "# --- More advanced Vocab usage ---\n",
    "# The `nlp.vocab.strings` object is a StringStore, which manages all string-to-ID mappings.\n",
    "print(\"\\nSpaCy Vocab Example:\")\n",
    "print(\"Current vocab size:\", len(nlp.vocab))\n",
    "\n",
    "# Add a new word to the vocabulary and Get the ID of the new word\n",
    "new_word = \"neural_network\"\n",
    "nlp.vocab.strings.add(new_word)\n",
    "new_word_id = nlp.vocab.strings.as_int(new_word)\n",
    "print(f\"Added '{new_word}', its ID is: {new_word_id}\")\n",
    "\n",
    "# Decode the ID back to the string\n",
    "decoded_new_word = nlp.vocab.strings.as_string(new_word_id)\n",
    "print(f\"Decoding ID {new_word_id}: {decoded_new_word}\")\n",
    "\n",
    "# If I change the hash value...\n",
    "try:\n",
    "    modified_id = new_word_id + 1\n",
    "    modified_word = nlp.vocab.strings.as_string(modified_id)\n",
    "    print(f\"After Modification: {modified_word}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee303614",
   "metadata": {},
   "source": [
    "## Advanced Tokenizer\n",
    "\n",
    "We will use `tiktoken` for advanced tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903535fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Choose an encoding that matches the models you might be using (e.g., for GPT-4, GPT-3.5-turbo)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "text_en = \"Hello, world! This is a test sentence for tiktoken.\"\n",
    "\n",
    "# --- Encode (Text to Token IDs) ---\n",
    "# encode() converts the string into a list of integer token IDs\n",
    "token_ids_en = encoding.encode(text_en)\n",
    "print(f\"Original English Text: '{text_en}'\")\n",
    "print(f\"Encoded Token IDs (English): {token_ids_en}\")\n",
    "print(f\"Number of tokens (English): {len(token_ids_en)}\")\n",
    "\n",
    "# To see the actual tokens that correspond to the IDs (optional, for understanding)\n",
    "# This requires decoding each ID individually or using a helper.\n",
    "# Note: decoding individual IDs might not always yield readable strings if tokens are sub-word units.\n",
    "decoded_parts_en = [encoding.decode([token_id]) for token_id in token_ids_en]\n",
    "print(f\"Decoded Parts (English, for understanding): {decoded_parts_en}\")\n",
    "\n",
    "# --- Decode (Token IDs to Text) ---\n",
    "# decode() converts a list of integer token IDs back into a string\n",
    "decoded_text_en = encoding.decode(token_ids_en)\n",
    "print(f\"Decoded English Text: '{decoded_text_en}'\")\n",
    "\n",
    "decoded_modified = [encoding.decode([token_id + 1]) for token_id in token_ids_en]\n",
    "print(decoded_modified)\n",
    "\n",
    "# Check if decoded text matches original (should be True)\n",
    "print(f\"Decoded matches original? {decoded_text_en == text_en}\")\n",
    "print(f\"After Modifications? {decoded_modified == text_en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204439f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "# tiktoken is for byte-pair encoding (BPE), Chinese is not supported.\n",
    "\n",
    "test_strings = [\n",
    "    \"Hello world, My name is Xiyuan Yang\",\n",
    "    \"wow, it is so fantastic!\",\n",
    "    \"你好，这里是中文，自古逢秋悲寂寥，我言秋日胜春朝\",\n",
    "    \"international computational\"\n",
    "]\n",
    "with open(\"../../README.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    long_string = file.read()\n",
    "    file.close()\n",
    "# test_strings.append(long_string)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for test_string in test_strings:\n",
    "    tokens = encoding.encode(test_string)\n",
    "    num_bytes = len(bytes(test_string, encoding=\"utf-8\"))\n",
    "    # print(num_bytes)\n",
    "    num_tokens = len(tokens)\n",
    "    decoded = [encoding.decode([token]) for token in tokens]\n",
    "    print(decoded)\n",
    "    print(num_bytes / num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510776e0",
   "metadata": {},
   "source": [
    "Let's analyse the result:\n",
    "\n",
    "$$\\text{evluation} = \\frac{\\text{num bytes}}{\\text{num tokens}}$$\n",
    "\n",
    "- For ANSCI, one single character is 1 bytes. (`char` in C++)\n",
    "\n",
    "- For Unicode, one single character is 3 bytes.\n",
    "\n",
    "```text\n",
    "['Hello', ' world', ',', ' My', ' name', ' is', ' X', 'iy', 'uan', ' Yang']\n",
    "3.5\n",
    "['wow', ',', ' it', ' is', ' so', ' fantastic', '!']\n",
    "3.4285714285714284\n",
    "['你', '好', '，', '这', '里', '是', '中', '文', '，', '自', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '，', '我', '言', '�', '�', '日', '�', '�', '�', '�', '�', '�']\n",
    "2.057142857142857\n",
    "['international', ' computational']\n",
    "13.5\n",
    "```\n",
    "\n",
    "- For the English word, the tokenization process is based on **words**, so `num_bytes/num_tokens` is mainly based on the average length of words.\n",
    "\n",
    "- For the Chinese word, the tokenization process is based on **single characters**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed97e2",
   "metadata": {},
   "source": [
    "## Principles\n",
    "\n",
    "### Character-based Tokenization\n",
    "\n",
    "For $\\mathbb{Y} \\subseteq \\mathcal{P}(M)$ is finite, there exists at least one hash function that satisfy all the requirements above. A Unicode string is a sequence of Unicode characters. Each character can be converted into a code point (integer) via `ord`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0a9f067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]\n",
      "20320\n",
      "44\n",
      "32\n",
      "9989\n",
      "ord() expected a character, but string of length 11 found\n",
      "✅\n",
      "你\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "ord_result = [ord(letter) for letter in string.ascii_letters]\n",
    "print(ord_result)\n",
    "\n",
    "print(ord(\"你\"))\n",
    "\n",
    "print(ord(\",\"))\n",
    "print(ord(\" \"))\n",
    "print(ord(\"✅\"))\n",
    "\n",
    "try:\n",
    "    print(ord(\"Hello world\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "print(chr(9989))\n",
    "print(chr(20320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20864ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Tokenizer(ABC):\n",
    "    \"\"\"Abstract interface for a tokenizer.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CharacterTokenizer(Tokenizer):\n",
    "    \"\"\"Represent a string as a sequence of Unicode code points.\"\"\"\n",
    "\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        return list(map(ord, string))\n",
    "\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return \"\".join(map(chr, indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71f13f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33258, 21476, 36898, 31179, 24754, 23490, 23525, 65292, 25105, 35328, 31179, 26085, 32988, 26149, 26397]\n",
      "自古逢秋悲寂寥，我言秋日胜春朝\n",
      "65293\n"
     ]
    }
   ],
   "source": [
    "string = \"自古逢秋悲寂寥，我言秋日胜春朝\"\n",
    "tokenizer = CharacterTokenizer()\n",
    "\n",
    "indices = tokenizer.encode(string)  # @inspect indices\n",
    "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n",
    "\n",
    "print(indices)\n",
    "print(reconstructed_string)\n",
    "\n",
    "assert string == reconstructed_string\n",
    "vocabulary_size = max(indices) + 1\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523886f1",
   "metadata": {},
   "source": [
    "#### Byte-based Tokenization\n",
    "\n",
    "Unicode strings can be represented as a sequence of bytes, which can be represented by integers between 0 and 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a2d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a'\n",
      "b'\\xe8\\x87\\xaa\\xe5\\x8f\\xa4\\xe9\\x80\\xa2\\xe7\\xa7\\x8b\\xe6\\x82\\xb2\\xe5\\xaf\\x82\\xe5\\xaf\\xa5\\xef\\xbc\\x8c\\xe6\\x88\\x91\\xe8\\xa8\\x80\\xe7\\xa7\\x8b\\xe6\\x97\\xa5\\xe8\\x83\\x9c\\xe6\\x98\\xa5\\xe6\\x9c\\x9d'\n"
     ]
    }
   ],
   "source": [
    "print(bytes(\"a\", encoding=\"utf-8\"))\n",
    "print(bytes(\"自古逢秋悲寂寥，我言秋日胜春朝\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "714fe7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[232, 135, 170, 229, 143, 164, 233, 128, 162, 231, 167, 139, 230, 130, 178, 229, 175, 130, 229, 175, 165, 239, 188, 140, 230, 136, 145, 232, 168, 128, 231, 167, 139, 230, 151, 165, 232, 131, 156, 230, 152, 165, 230, 156, 157]\n",
      "自古逢秋悲寂寥，我言秋日胜春朝\n"
     ]
    }
   ],
   "source": [
    "class ByteTokenizer(Tokenizer):\n",
    "    \"\"\"Represent a string as a sequence of bytes.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        string_bytes = string.encode(\"utf-8\")  # @inspect string_bytes\n",
    "        indices = list(map(int, string_bytes))  # @inspect indices\n",
    "        return indices\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        string_bytes = bytes(indices)  # @inspect string_bytes\n",
    "        string = string_bytes.decode(\"utf-8\")  # @inspect string\n",
    "        return string\n",
    "\n",
    "tokenizer = ByteTokenizer()\n",
    "print(tokenizer.encode(\"自古逢秋悲寂寥，我言秋日胜春朝\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"自古逢秋悲寂寥，我言秋日胜春朝\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b42583b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', ' ', 'welcome', ' ', 'to', ' ', 'the', ' ', 'world', ' ', 'of', ' ', 'large', ' ', 'language', ' ', 'models', '!']\n",
      "['Hello', ',', ' welcome', ' to', ' the', ' world', ' of', ' large', ' language', ' models', '!']\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "\n",
    "GPT2_TOKENIZER_REGEX = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "string = \"Hello, welcome to the world of large language models!\"\n",
    "segments = regex.findall(r\"\\w+|.\", string)\n",
    "print(segments)\n",
    "\n",
    "pattern = GPT2_TOKENIZER_REGEX\n",
    "segments = regex.findall(pattern, string)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0f9c1",
   "metadata": {},
   "source": [
    "### BPE Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "def merge(\n",
    "    indices: list[int], pair: tuple[int, int], new_index: int\n",
    ") -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index\n",
    "    \"\"\"Return `indices`, but with all instances of `pair` replaced with `new_index`.\"\"\"\n",
    "    new_indices = []  # @inspect new_indices\n",
    "    i = 0  # @inspect i\n",
    "    while i < len(indices):\n",
    "        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:\n",
    "            new_indices.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_indices.append(indices[i])\n",
    "            i += 1\n",
    "    # update the whole indices, thus it is very low\n",
    "    return new_indices\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n",
    "\n",
    "    vocab: dict[int, bytes]  # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1, index2 -> new_index\n",
    "\n",
    "class BPETokenizer(Tokenizer):\n",
    "    \"\"\"BPE tokenizer given a set of merges and a vocabulary.\"\"\"\n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices\n",
    "        # Note: this is a very slow implementation\n",
    "        # simulate the whole merging process\n",
    "        for pair, new_index in self.params.merges.items():  # @inspect pair, @inspect new_index\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        bytes_list = list(map(self.params.vocab.get, indices))  # @inspect bytes_list\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")  # @inspect string\n",
    "        return string\n",
    "\n",
    "def train_bpe(\n",
    "    string: str, num_merges: int\n",
    ") -> BPETokenizerParams:  # @inspect string, @inspect num_merges\n",
    "    # Start with the list of bytes of string.\n",
    "    indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices\n",
    "    merges: dict[tuple[int, int], int] = {}  # index1, index2 => merged index\n",
    "\n",
    "    # initial vocab\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes\n",
    "\n",
    "    for i in range(num_merges):\n",
    "        # Count the number of occurrences of each pair of tokens\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        # !really pythonic!\n",
    "        for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair\n",
    "            counts[(index1, index2)] += 1  # @inspect counts\n",
    "        # Find the most common pair.\n",
    "        pair = max(counts, key=counts.get)  # @inspect pair\n",
    "        index1, index2 = pair\n",
    "        # Merge that pair.\n",
    "        new_index = 256 + i  # @inspect new_index\n",
    "        merges[pair] = new_index  # @inspect merges\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]  # @inspect vocab\n",
    "        indices = merge(indices, pair, new_index)  # @inspect indices\n",
    "    return BPETokenizerParams(vocab=vocab, merges=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPETokenizerParams(vocab={0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'th', 257: b'the', 258: b'the '}, merges={(116, 104): 256, (256, 101): 257, (257, 32): 258})\n",
      "[258, 113, 117, 105, 99, 107, 32, 98, 114, 111, 119, 110, 32, 102, 111, 120]\n"
     ]
    }
   ],
   "source": [
    "string = \"the cat in the hat\"  # @inspect string\n",
    "params = train_bpe(string, num_merges=3)\n",
    "print(params)\n",
    "\n",
    "tokenizer = BPETokenizer(params)\n",
    "string = \"the quick brown fox\"  # @inspect string\n",
    "indices = tokenizer.encode(string)  # @inspect indices\n",
    "print(indices)\n",
    "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n",
    "assert string == reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35bb240c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[258, 113, 117, 105, 99, 107, 32, 98, 114, 111, 119, 110, 32, 102, 111, 120]\n",
      "[271, 261, 265, 267]\n",
      "the \n",
      "q\n",
      "u\n",
      "i\n",
      "c\n",
      "k\n",
      " \n",
      "b\n",
      "r\n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "f\n",
      "o\n",
      "x\n",
      "\n",
      "\n",
      "the \n",
      "quick \n",
      "brown \n",
      "fox\n"
     ]
    }
   ],
   "source": [
    "# if you use different traning data?\n",
    "\n",
    "params_2 = train_bpe(\"quick quick brown brown fox fox the the hfbdn ghdnb hfj\", num_merges=20)\n",
    "tokenizer_2 = BPETokenizer(params_2)\n",
    "indices_2 = tokenizer_2.encode(string)\n",
    "\n",
    "print(indices)\n",
    "print(indices_2)\n",
    "\n",
    "for index in indices:\n",
    "    print(tokenizer.decode([index]))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for index in indices_2:\n",
    "    print(tokenizer_2.decode([index]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
