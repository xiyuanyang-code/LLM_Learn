{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d98563",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eba875f",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "\n",
    "Install `nltk` and download its data.\n",
    "\n",
    "```bash\n",
    "pip install nltk\n",
    "python -m nltk.downloader punkt\n",
    "```\n",
    "\n",
    "Then it will download the data in the home directory:\n",
    "```text\n",
    "[nltk_data] Downloading package punkt to\n",
    "[nltk_data]     /GPFS/rhome/xiyuanyang/nltk_data...\n",
    "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861066e",
   "metadata": {},
   "source": [
    "## Quick Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# step1: download some data\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "text = \"This is an example sentence, with punctuation!\"\n",
    "words = word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "# for some chinese demo\n",
    "text_chn = \"我是上海交通大学的一名学生,你觉的我怎么样，I am very happy to see you!\"\n",
    "# nltk is quite foolish...\n",
    "words_chn = word_tokenize(text_chn)\n",
    "print(words_chn)\n",
    "\n",
    "# for chinses, you can use jieba\n",
    "import jieba\n",
    "\n",
    "\n",
    "text = \"我爱北京天安门，天安门上太阳升。\"\n",
    "# 精确模式\n",
    "seg_list_precise = jieba.cut(text, cut_all=False)\n",
    "print(\"精确模式:\", \" \".join(seg_list_precise))\n",
    "\n",
    "# 全模式\n",
    "seg_list_all = jieba.cut(text, cut_all=True)\n",
    "print(\"全模式:\", \" \".join(seg_list_all))\n",
    "\n",
    "# 搜索引擎模式\n",
    "seg_list_search = jieba.cut_for_search(text)\n",
    "print(\"搜索引擎模式:\", \" \".join(seg_list_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8357dcb",
   "metadata": {},
   "source": [
    "## Encoding and Decoding\n",
    "\n",
    "We will use `spacy` for more advanced NLP usage.\n",
    "\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "```text\n",
    "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "Collecting en-core-web-sm==3.8.0\n",
    "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
    "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 728.1 kB/s eta 0:00:00\n",
    "Installing collected packages: en-core-web-sm\n",
    "Successfully installed en-core-web-sm-3.8.0\n",
    "✔ Download and installation successful\n",
    "You can now load the package via spacy.load('en_core_web_sm')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27afc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"This is an example sentence, with punctuation!\"\n",
    "doc = nlp(text)\n",
    "\n",
    "words = [token.text for token in doc]\n",
    "print(words)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} {token.pos_:<10} {token.is_punct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11958afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for encoding and decoding\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Hello world, I am Xiyuan Yang, a freshman in Shanghai JiaoTong University.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get the tokenized words (tokens)\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"SpaCy Tokens:\", tokens)\n",
    "\n",
    "# --- Encode (Get Token Hashes or IDs) ---\n",
    "# spaCy internally uses hash values to represent strings, and these hashes map to integer IDs.\n",
    "# token.norm is the normalized hash value (lowercase, etc.)\n",
    "# token.orth is the hash value of the original string (case-sensitive).\n",
    "\n",
    "token_ids = [nlp.vocab.strings.as_int(token.text) for token in doc]\n",
    "print(\"Token IDs (encoded):\", token_ids)\n",
    "\n",
    "\n",
    "# --- Decode (Convert IDs back to Tokens) ---\n",
    "decoded_tokens = [nlp.vocab.strings.as_string(id_val) for id_val in token_ids]\n",
    "print(\"Decoded Tokens:\", decoded_tokens)\n",
    "\n",
    "# --- More advanced Vocab usage ---\n",
    "# The `nlp.vocab.strings` object is a StringStore, which manages all string-to-ID mappings.\n",
    "print(\"\\nSpaCy Vocab Example:\")\n",
    "print(\"Current vocab size:\", len(nlp.vocab))\n",
    "\n",
    "# Add a new word to the vocabulary and Get the ID of the new word\n",
    "new_word = \"neural_network\"\n",
    "nlp.vocab.strings.add(new_word)\n",
    "new_word_id = nlp.vocab.strings.as_int(new_word)\n",
    "print(f\"Added '{new_word}', its ID is: {new_word_id}\")\n",
    "\n",
    "# Decode the ID back to the string\n",
    "decoded_new_word = nlp.vocab.strings.as_string(new_word_id)\n",
    "print(f\"Decoding ID {new_word_id}: {decoded_new_word}\")\n",
    "\n",
    "# If I change the hash value...\n",
    "try:\n",
    "    modified_id = new_word_id + 1\n",
    "    modified_word = nlp.vocab.strings.as_string(modified_id)\n",
    "    print(f\"After Modification: {modified_word}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee303614",
   "metadata": {},
   "source": [
    "## Advanced Tokenizer\n",
    "\n",
    "We will use `tiktoken` for advanced tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903535fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Choose an encoding that matches the models you might be using (e.g., for GPT-4, GPT-3.5-turbo)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "text_en = \"Hello, world! This is a test sentence for tiktoken.\"\n",
    "\n",
    "# --- Encode (Text to Token IDs) ---\n",
    "# encode() converts the string into a list of integer token IDs\n",
    "token_ids_en = encoding.encode(text_en)\n",
    "print(f\"Original English Text: '{text_en}'\")\n",
    "print(f\"Encoded Token IDs (English): {token_ids_en}\")\n",
    "print(f\"Number of tokens (English): {len(token_ids_en)}\")\n",
    "\n",
    "# To see the actual tokens that correspond to the IDs (optional, for understanding)\n",
    "# This requires decoding each ID individually or using a helper.\n",
    "# Note: decoding individual IDs might not always yield readable strings if tokens are sub-word units.\n",
    "decoded_parts_en = [encoding.decode([token_id]) for token_id in token_ids_en]\n",
    "print(f\"Decoded Parts (English, for understanding): {decoded_parts_en}\")\n",
    "\n",
    "# --- Decode (Token IDs to Text) ---\n",
    "# decode() converts a list of integer token IDs back into a string\n",
    "decoded_text_en = encoding.decode(token_ids_en)\n",
    "print(f\"Decoded English Text: '{decoded_text_en}'\")\n",
    "\n",
    "decoded_modified = [encoding.decode([token_id + 1]) for token_id in token_ids_en]\n",
    "print(decoded_modified)\n",
    "\n",
    "# Check if decoded text matches original (should be True)\n",
    "print(f\"Decoded matches original? {decoded_text_en == text_en}\")\n",
    "print(f\"After Modifications? {decoded_modified == text_en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204439f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "# tiktoken is for byte-pair encoding (BPE), Chinese is not supported.\n",
    "\n",
    "test_strings = [\n",
    "    \"Hello world, My name is Xiyuan Yang\",\n",
    "    \"wow, it is so fantastic!\",\n",
    "    \"你好，这里是中文，自古逢秋悲寂寥，我言秋日胜春朝\",\n",
    "    \"international computational\"\n",
    "]\n",
    "with open(\"../../README.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    long_string = file.read()\n",
    "    file.close()\n",
    "# test_strings.append(long_string)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for test_string in test_strings:\n",
    "    tokens = encoding.encode(test_string)\n",
    "    num_bytes = len(bytes(test_string, encoding=\"utf-8\"))\n",
    "    # print(num_bytes)\n",
    "    num_tokens = len(tokens)\n",
    "    decoded = [encoding.decode([token]) for token in tokens]\n",
    "    print(decoded)\n",
    "    print(num_bytes / num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510776e0",
   "metadata": {},
   "source": [
    "Let's analyse the result:\n",
    "\n",
    "$$\\text{evluation} = \\frac{\\text{num bytes}}{\\text{num tokens}}$$\n",
    "\n",
    "- For ANSCI, one single character is 1 bytes. (`char` in C++)\n",
    "\n",
    "- For Unicode, one single character is 3 bytes.\n",
    "\n",
    "```text\n",
    "['Hello', ' world', ',', ' My', ' name', ' is', ' X', 'iy', 'uan', ' Yang']\n",
    "3.5\n",
    "['wow', ',', ' it', ' is', ' so', ' fantastic', '!']\n",
    "3.4285714285714284\n",
    "['你', '好', '，', '这', '里', '是', '中', '文', '，', '自', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '，', '我', '言', '�', '�', '日', '�', '�', '�', '�', '�', '�']\n",
    "2.057142857142857\n",
    "['international', ' computational']\n",
    "13.5\n",
    "```\n",
    "\n",
    "- For the English word, the tokenization process is based on **words**, so `num_bytes/num_tokens` is mainly based on the average length of words.\n",
    "\n",
    "- For the Chinese word, the tokenization process is based on **single characters**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed97e2",
   "metadata": {},
   "source": [
    "## Principles\n",
    "\n",
    "### Character-based Tokenization\n",
    "\n",
    "For $\\mathbb{Y} \\subseteq \\mathcal{P}(M)$ is finite, there exists at least one hash function that satisfy all the requirements above. A Unicode string is a sequence of Unicode characters. Each character can be converted into a code point (integer) via `ord`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "ord_result = [ord(letter) for letter in string.ascii_letters]\n",
    "print(ord_result)\n",
    "\n",
    "print(ord(\"你\"))\n",
    "\n",
    "print(ord(\",\"))\n",
    "print(ord(\" \"))\n",
    "print(ord(\"✅\"))\n",
    "\n",
    "try:\n",
    "    print(ord(\"Hello world\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "print(chr(9989))\n",
    "print(chr(20320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20864ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Tokenizer(ABC):\n",
    "    \"\"\"Abstract interface for a tokenizer.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CharacterTokenizer(Tokenizer):\n",
    "    \"\"\"Represent a string as a sequence of Unicode code points.\"\"\"\n",
    "\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        return list(map(ord, string))\n",
    "\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return \"\".join(map(chr, indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f13f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"自古逢秋悲寂寥，我言秋日胜春朝\"\n",
    "tokenizer = CharacterTokenizer()\n",
    "\n",
    "indices = tokenizer.encode(string)  # @inspect indices\n",
    "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n",
    "\n",
    "print(indices)\n",
    "print(reconstructed_string)\n",
    "\n",
    "assert string == reconstructed_string\n",
    "vocabulary_size = max(indices) + 1\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523886f1",
   "metadata": {},
   "source": [
    "#### Byte-based Tokenization\n",
    "\n",
    "Unicode strings can be represented as a sequence of bytes, which can be represented by integers between 0 and 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bytes(\"a\", encoding=\"utf-8\"))\n",
    "print(bytes(\"自古逢秋悲寂寥，我言秋日胜春朝\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714fe7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteTokenizer(Tokenizer):\n",
    "    \"\"\"Represent a string as a sequence of bytes.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        string_bytes = string.encode(\"utf-8\")  # @inspect string_bytes\n",
    "        indices = list(map(int, string_bytes))  # @inspect indices\n",
    "        return indices\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        string_bytes = bytes(indices)  # @inspect string_bytes\n",
    "        string = string_bytes.decode(\"utf-8\")  # @inspect string\n",
    "        return string\n",
    "\n",
    "tokenizer = ByteTokenizer()\n",
    "print(tokenizer.encode(\"自古逢秋悲寂寥，我言秋日胜春朝\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"自古逢秋悲寂寥，我言秋日胜春朝\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "GPT2_TOKENIZER_REGEX = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "string = \"Hello, welcome to the world of large language models!\"\n",
    "segments = regex.findall(r\"\\w+|.\", string)\n",
    "print(segments)\n",
    "\n",
    "pattern = GPT2_TOKENIZER_REGEX\n",
    "segments = regex.findall(pattern, string)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0f9c1",
   "metadata": {},
   "source": [
    "### BPE Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "def merge(\n",
    "    indices: list[int], pair: tuple[int, int], new_index: int\n",
    ") -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index\n",
    "    \"\"\"Return `indices`, but with all instances of `pair` replaced with `new_index`.\"\"\"\n",
    "    new_indices = []  # @inspect new_indices\n",
    "    i = 0  # @inspect i\n",
    "    while i < len(indices):\n",
    "        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:\n",
    "            new_indices.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_indices.append(indices[i])\n",
    "            i += 1\n",
    "    # update the whole indices, thus it is very low\n",
    "    return new_indices\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n",
    "\n",
    "    vocab: dict[int, bytes]  # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1, index2 -> new_index\n",
    "\n",
    "class BPETokenizer(Tokenizer):\n",
    "    \"\"\"BPE tokenizer given a set of merges and a vocabulary.\"\"\"\n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices\n",
    "        # Note: this is a very slow implementation\n",
    "        # simulate the whole merging process\n",
    "        for pair, new_index in self.params.merges.items():  # @inspect pair, @inspect new_index\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        bytes_list = list(map(self.params.vocab.get, indices))  # @inspect bytes_list\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")  # @inspect string\n",
    "        return string\n",
    "\n",
    "def train_bpe(\n",
    "    string: str, num_merges: int\n",
    ") -> BPETokenizerParams:  # @inspect string, @inspect num_merges\n",
    "    # Start with the list of bytes of string.\n",
    "    indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices\n",
    "    merges: dict[tuple[int, int], int] = {}  # index1, index2 => merged index\n",
    "\n",
    "    # initial vocab\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes\n",
    "\n",
    "    for i in range(num_merges):\n",
    "        # Count the number of occurrences of each pair of tokens\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        # !really pythonic!\n",
    "        for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair\n",
    "            counts[(index1, index2)] += 1  # @inspect counts\n",
    "        # Find the most common pair.\n",
    "        pair = max(counts, key=counts.get)  # @inspect pair\n",
    "        index1, index2 = pair\n",
    "        # Merge that pair.\n",
    "        new_index = 256 + i  # @inspect new_index\n",
    "        merges[pair] = new_index  # @inspect merges\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]  # @inspect vocab\n",
    "        indices = merge(indices, pair, new_index)  # @inspect indices\n",
    "    return BPETokenizerParams(vocab=vocab, merges=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"the cat in the hat\"  # @inspect string\n",
    "params = train_bpe(string, num_merges=3)\n",
    "print(params)\n",
    "\n",
    "tokenizer = BPETokenizer(params)\n",
    "string = \"the quick brown fox\"  # @inspect string\n",
    "indices = tokenizer.encode(string)  # @inspect indices\n",
    "print(indices)\n",
    "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n",
    "assert string == reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you use different traning data?\n",
    "\n",
    "params_2 = train_bpe(\"quick quick brown brown fox fox the the hfbdn ghdnb hfj\", num_merges=20)\n",
    "tokenizer_2 = BPETokenizer(params_2)\n",
    "indices_2 = tokenizer_2.encode(string)\n",
    "\n",
    "print(indices)\n",
    "print(indices_2)\n",
    "\n",
    "for index in indices:\n",
    "    print(tokenizer.decode([index]))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for index in indices_2:\n",
    "    print(tokenizer_2.decode([index]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
